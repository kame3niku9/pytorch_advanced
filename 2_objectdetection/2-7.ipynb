{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 学習と検証の実装"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as data\n",
    "import torch.nn.init as init\n",
    "import torch.optim as optim\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import time\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 1234\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataLoaderの作成\n",
    "\n",
    "from utils.ssd_model import make_datapath_list, VOCDataset, DataTransform, Anno_xml2list, od_collate_fn\n",
    "\n",
    "rootpath = \"./data/VOCdevkit/VOC2012/\"\n",
    "train_img_list, train_anno_list, val_img_list, val_anno_list = make_datapath_list(rootpath)\n",
    "\n",
    "voc_classes = ['aeroplane', 'bicycle', 'bird', 'boat',\n",
    "               'bottle', 'bus', 'car', 'cat', 'chair',\n",
    "               'cow', 'diningtable', 'dog', 'horse',\n",
    "               'motorbike', 'person', 'pottedplant',\n",
    "               'sheep', 'sofa', 'train', 'tvmonitor']\n",
    "\n",
    "color_mean = (104, 117, 123)\n",
    "input_size = 300\n",
    "\n",
    "train_dataset = VOCDataset(train_img_list, train_anno_list, phase='train', \n",
    "                           transform=DataTransform(input_size, color_mean), transform_anno=Anno_xml2list(voc_classes))\n",
    "val_dataset = VOCDataset(val_img_list, val_anno_list, phase='val',\n",
    "                        transform=DataTransform(input_size, color_mean), transform_anno=Anno_xml2list(voc_classes))\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "train_dataloader = data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=od_collate_fn)\n",
    "val_dataloader = data.DataLoader(val_dataset, batch_size=batch_size, shuffle=False, collate_fn=od_collate_fn)\n",
    "\n",
    "dataloaders_dict = {'train': train_dataloader, 'val': val_dataloader}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "使用デバイス: cuda:0\n"
     ]
    }
   ],
   "source": [
    "# ネットワークモデルの作成\n",
    "\n",
    "from utils.ssd_model import SSD\n",
    "\n",
    "ssd_cfg = {\n",
    "    'num_classes': 21, # 背景クラスも含む\n",
    "    'input_size': 300,\n",
    "    'bbox_aspect_num': [4, 6, 6, 6, 4, 4], # DBoxのアスペクト比の種類\n",
    "    'feature_maps': [38, 19, 10, 5, 3, 1], # 各sourceの画像サイズ\n",
    "    'steps': [8, 16, 32, 64, 100, 300], # DBoxの大きさを決める\n",
    "    'min_sizes': [30, 60, 111, 162, 213, 264], # DBoxの大きさを決める\n",
    "    'max_sizes': [60, 111, 162, 213, 264, 315], # DBoxの大きさを決める\n",
    "    'aspect_ratios': [[2], [2, 3], [2, 3], [2, 3], [2], [2]],\n",
    "}\n",
    "\n",
    "net = SSD(phase='train', cfg=ssd_cfg)\n",
    "\n",
    "# 初期値の重みの設定\n",
    "vgg_weights = torch.load('./weights/vgg16_reducedfc.pth')\n",
    "net.vgg.load_state_dict(vgg_weights)\n",
    "\n",
    "# その他はHeの初期値に\n",
    "def weights_init(m):\n",
    "    if isinstance(m, nn.Conv2d):\n",
    "        init.kaiming_normal_(m.weight.data)\n",
    "        if m.bias is not None:\n",
    "            nn.init.constant_(m.bias, 0.0)\n",
    "            \n",
    "# Heの初期値を適用\n",
    "net.extras.apply(weights_init)\n",
    "net.loc.apply(weights_init)\n",
    "net.conf.apply(weights_init)\n",
    "\n",
    "print(f\"使用デバイス: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 損失関数と最適化手法の設定\n",
    "from utils.ssd_model import MultiBoxLoss\n",
    "\n",
    "criterion = MultiBoxLoss(jaccard_thresh=0.5, neg_pos=3, device=device)\n",
    "\n",
    "optimizer = optim.SGD(net.parameters(), lr=1e-3, momentum=0.9, weight_decay=5e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 学習と検証の実施\n",
    "def train_model(net, dataloaders_dict, criterion, optimizer, num_epochs):\n",
    "    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f'使用デバイス: {device}')\n",
    "    \n",
    "    net.to(device)\n",
    "    \n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    \n",
    "    save_epoch_cycle = 10\n",
    "    iteration = 1\n",
    "    epoch_train_loss = 0.0\n",
    "    epoch_val_loss = 0.0\n",
    "    logs = []\n",
    "    \n",
    "    for epoch in range(num_epochs + 1):\n",
    "        t_epoch_start = time.time()\n",
    "        t_iter_start = time.time()\n",
    "        \n",
    "        print('------------')\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}')\n",
    "        print('------------')\n",
    "        \n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                net.train()\n",
    "                print(' (train) ')\n",
    "            else:\n",
    "                if ((epoch+1) % save_epoch_cycle == 0):\n",
    "                    net.eval()\n",
    "                    print(' (val) ')\n",
    "                else:\n",
    "                    continue\n",
    "                    \n",
    "            for images, targets in dataloaders_dict[phase]:\n",
    "                images = images.to(device)\n",
    "                targets = [ann.to(device) for ann in targets] # リストの各要素のテンソルをGPUへ\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                with torch.set_grad_enabled(phase=='train'):\n",
    "                    outputs = net(images)\n",
    "                    \n",
    "                    loss_l, loss_c = criterion(outputs, targets)\n",
    "                    loss = loss_l + loss_c\n",
    "                    \n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        nn.utils.clip_grad_value_(net.parameters(), clip_value=2.0)\n",
    "                        optimizer.step()\n",
    "                        \n",
    "                        if (iteration % save_epoch_cycle == 0):\n",
    "                            t_iter_finish = time.time()\n",
    "                            duration = t_iter_finish - t_iter_start\n",
    "                            print(f'iteration {iteration} || Loss: {loss.item():.4f} || 10iter: {duration:.4f} sec')\n",
    "                            \n",
    "                        epoch_train_loss += loss.item()\n",
    "                        iteration += 1\n",
    "                        \n",
    "                    else:\n",
    "                        epoch_val_loss += loss.item()\n",
    "                        \n",
    "        t_epoch_finish = time.time()\n",
    "        print('------------')\n",
    "        print(f'epoch {epoch+1} || Epoch_TRAIN_Loss: {epoch_train_loss:.4f} || Epoch_VAL_Loss: {epoch_val_loss:.4f}')\n",
    "        print(f'timer: {t_epoch_finish - t_epoch_start:.4f}sec')\n",
    "        t_epoch_start = time.time()\n",
    "\n",
    "        log_epoch = {'epoch': epoch+1, 'train_loss': epoch_train_loss, 'val_loss': epoch_val_loss}\n",
    "        logs.append(log_epoch)\n",
    "        df = pd.DataFrame(logs)\n",
    "        df.to_csv(\"log_output.csv\")\n",
    "\n",
    "        epoch_train_loss = 0.0\n",
    "        epoch_val_loss = 0.0\n",
    "\n",
    "        if ((epoch+1) % save_epoch_cycle == 0):\n",
    "            torch.save(net.state_dict(), 'weights/ssd300_' + str(epoch+1) + '.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "使用デバイス: cuda:0\n",
      "------------\n",
      "Epoch 1/50\n",
      "------------\n",
      " (train) \n",
      "iteration 10 || Loss: 4.7485 || 10iter: 8.3590 sec\n",
      "iteration 20 || Loss: 4.6855 || 10iter: 17.8754 sec\n",
      "iteration 30 || Loss: 5.1709 || 10iter: 26.4507 sec\n",
      "iteration 40 || Loss: 4.5907 || 10iter: 35.1732 sec\n",
      "iteration 50 || Loss: 4.7877 || 10iter: 44.3832 sec\n",
      "iteration 60 || Loss: 4.0540 || 10iter: 53.6042 sec\n",
      "iteration 70 || Loss: 4.6202 || 10iter: 63.3255 sec\n",
      "iteration 80 || Loss: 4.5606 || 10iter: 72.2908 sec\n",
      "iteration 90 || Loss: 4.3560 || 10iter: 81.3396 sec\n",
      "iteration 100 || Loss: 4.4810 || 10iter: 90.3114 sec\n",
      "iteration 110 || Loss: 5.1089 || 10iter: 100.3378 sec\n",
      "iteration 120 || Loss: 4.8257 || 10iter: 109.4112 sec\n",
      "iteration 130 || Loss: 5.1575 || 10iter: 118.7993 sec\n",
      "iteration 140 || Loss: 4.1586 || 10iter: 127.7272 sec\n",
      "iteration 150 || Loss: 4.6103 || 10iter: 136.7313 sec\n",
      "iteration 160 || Loss: 3.9928 || 10iter: 146.9780 sec\n",
      "iteration 170 || Loss: 4.9134 || 10iter: 156.5555 sec\n",
      "------------\n",
      "epoch 1 || Epoch_TRAIN_Loss: 843.2739 || Epoch_VAL_Loss: 0.0000\n",
      "timer: 166.0019sec\n",
      "------------\n",
      "Epoch 2/50\n",
      "------------\n",
      " (train) \n",
      "iteration 180 || Loss: 5.4342 || 10iter: 0.7822 sec\n",
      "iteration 190 || Loss: 4.5478 || 10iter: 9.8151 sec\n",
      "iteration 200 || Loss: 4.1983 || 10iter: 18.7786 sec\n",
      "iteration 210 || Loss: 4.6843 || 10iter: 27.8310 sec\n",
      "iteration 220 || Loss: 5.3356 || 10iter: 37.3479 sec\n",
      "iteration 230 || Loss: 4.5478 || 10iter: 46.3234 sec\n",
      "iteration 240 || Loss: 4.5141 || 10iter: 56.8583 sec\n",
      "iteration 250 || Loss: 4.4358 || 10iter: 65.8927 sec\n",
      "iteration 260 || Loss: 4.5347 || 10iter: 74.8818 sec\n",
      "iteration 270 || Loss: 4.8001 || 10iter: 84.3767 sec\n",
      "iteration 280 || Loss: 4.4840 || 10iter: 94.3188 sec\n",
      "iteration 290 || Loss: 5.3774 || 10iter: 104.3964 sec\n",
      "iteration 300 || Loss: 4.6337 || 10iter: 113.1820 sec\n",
      "iteration 310 || Loss: 4.3219 || 10iter: 122.4214 sec\n",
      "iteration 320 || Loss: 4.5212 || 10iter: 131.7002 sec\n",
      "iteration 330 || Loss: 4.3270 || 10iter: 141.1697 sec\n",
      "iteration 340 || Loss: 4.2262 || 10iter: 149.8251 sec\n",
      "iteration 350 || Loss: 3.9572 || 10iter: 158.9437 sec\n",
      "------------\n",
      "epoch 2 || Epoch_TRAIN_Loss: 832.4433 || Epoch_VAL_Loss: 0.0000\n",
      "timer: 166.5654sec\n",
      "------------\n",
      "Epoch 3/50\n",
      "------------\n",
      " (train) \n",
      "iteration 360 || Loss: 4.6436 || 10iter: 1.6120 sec\n",
      "iteration 370 || Loss: 4.3988 || 10iter: 10.7123 sec\n",
      "iteration 380 || Loss: 4.9818 || 10iter: 19.8315 sec\n",
      "iteration 390 || Loss: 4.2019 || 10iter: 29.1760 sec\n",
      "iteration 400 || Loss: 4.1597 || 10iter: 38.0073 sec\n",
      "iteration 410 || Loss: 5.2107 || 10iter: 46.7925 sec\n",
      "iteration 420 || Loss: 3.7673 || 10iter: 55.4230 sec\n",
      "iteration 430 || Loss: 4.4348 || 10iter: 64.3905 sec\n",
      "iteration 440 || Loss: 4.7600 || 10iter: 74.1214 sec\n",
      "iteration 450 || Loss: 3.9119 || 10iter: 83.5884 sec\n",
      "iteration 460 || Loss: 4.4585 || 10iter: 92.1625 sec\n",
      "iteration 470 || Loss: 4.9670 || 10iter: 101.8842 sec\n",
      "iteration 480 || Loss: 4.2141 || 10iter: 111.4191 sec\n",
      "iteration 490 || Loss: 5.0656 || 10iter: 120.5232 sec\n",
      "iteration 500 || Loss: 4.5516 || 10iter: 130.3701 sec\n",
      "iteration 510 || Loss: 4.7259 || 10iter: 139.2252 sec\n",
      "iteration 520 || Loss: 4.6035 || 10iter: 148.1928 sec\n",
      "iteration 530 || Loss: 5.0020 || 10iter: 157.0692 sec\n",
      "------------\n",
      "epoch 3 || Epoch_TRAIN_Loss: 817.3382 || Epoch_VAL_Loss: 0.0000\n",
      "timer: 163.2872sec\n",
      "------------\n",
      "Epoch 4/50\n",
      "------------\n",
      " (train) \n",
      "iteration 540 || Loss: 4.5261 || 10iter: 2.6664 sec\n",
      "iteration 550 || Loss: 5.1409 || 10iter: 12.1634 sec\n",
      "iteration 560 || Loss: 4.5422 || 10iter: 21.3122 sec\n",
      "iteration 570 || Loss: 4.3523 || 10iter: 30.2705 sec\n",
      "iteration 580 || Loss: 3.9064 || 10iter: 39.7756 sec\n",
      "iteration 590 || Loss: 4.5847 || 10iter: 49.3558 sec\n",
      "iteration 600 || Loss: 4.9966 || 10iter: 59.1066 sec\n",
      "iteration 610 || Loss: 4.5285 || 10iter: 67.9139 sec\n",
      "iteration 620 || Loss: 4.0437 || 10iter: 77.8911 sec\n",
      "iteration 630 || Loss: 4.4253 || 10iter: 86.9610 sec\n",
      "iteration 640 || Loss: 4.2731 || 10iter: 96.2846 sec\n",
      "iteration 650 || Loss: 4.0315 || 10iter: 105.3148 sec\n",
      "iteration 660 || Loss: 4.3542 || 10iter: 114.6876 sec\n",
      "iteration 670 || Loss: 4.8547 || 10iter: 123.9879 sec\n",
      "iteration 680 || Loss: 4.5727 || 10iter: 133.6793 sec\n",
      "iteration 690 || Loss: 4.6464 || 10iter: 143.4248 sec\n",
      "iteration 700 || Loss: 4.8458 || 10iter: 153.2512 sec\n",
      "iteration 710 || Loss: 4.3336 || 10iter: 162.3361 sec\n",
      "------------\n",
      "epoch 4 || Epoch_TRAIN_Loss: 809.5757 || Epoch_VAL_Loss: 0.0000\n",
      "timer: 167.7177sec\n",
      "------------\n",
      "Epoch 5/50\n",
      "------------\n",
      " (train) \n",
      "iteration 720 || Loss: 4.3521 || 10iter: 3.4152 sec\n",
      "iteration 730 || Loss: 4.5365 || 10iter: 12.7233 sec\n",
      "iteration 740 || Loss: 4.3899 || 10iter: 22.1085 sec\n",
      "iteration 750 || Loss: 4.3898 || 10iter: 31.9740 sec\n",
      "iteration 760 || Loss: 5.1455 || 10iter: 42.0141 sec\n",
      "iteration 770 || Loss: 4.8635 || 10iter: 51.3949 sec\n",
      "iteration 780 || Loss: 4.9062 || 10iter: 60.6737 sec\n",
      "iteration 790 || Loss: 4.6228 || 10iter: 70.1669 sec\n",
      "iteration 800 || Loss: 4.2950 || 10iter: 78.5431 sec\n",
      "iteration 810 || Loss: 4.5430 || 10iter: 88.2853 sec\n",
      "iteration 820 || Loss: 4.4985 || 10iter: 97.1384 sec\n",
      "iteration 830 || Loss: 4.7179 || 10iter: 106.4027 sec\n",
      "iteration 840 || Loss: 5.0870 || 10iter: 115.5792 sec\n",
      "iteration 850 || Loss: 4.4100 || 10iter: 124.0612 sec\n",
      "iteration 860 || Loss: 4.8241 || 10iter: 132.8808 sec\n",
      "iteration 870 || Loss: 4.0258 || 10iter: 142.2196 sec\n",
      "iteration 880 || Loss: 4.7552 || 10iter: 151.9268 sec\n",
      "iteration 890 || Loss: 4.0488 || 10iter: 161.5487 sec\n",
      "------------\n",
      "epoch 5 || Epoch_TRAIN_Loss: 801.2540 || Epoch_VAL_Loss: 0.0000\n",
      "timer: 166.1287sec\n",
      "------------\n",
      "Epoch 6/50\n",
      "------------\n",
      " (train) \n",
      "iteration 900 || Loss: 4.3463 || 10iter: 4.1785 sec\n",
      "iteration 910 || Loss: 4.0346 || 10iter: 13.5509 sec\n",
      "iteration 920 || Loss: 4.2743 || 10iter: 23.9993 sec\n",
      "iteration 930 || Loss: 4.4040 || 10iter: 32.8700 sec\n",
      "iteration 940 || Loss: 4.5840 || 10iter: 42.0080 sec\n",
      "iteration 950 || Loss: 3.6479 || 10iter: 51.2405 sec\n",
      "iteration 960 || Loss: 4.6449 || 10iter: 59.7003 sec\n",
      "iteration 970 || Loss: 4.0616 || 10iter: 68.9165 sec\n",
      "iteration 980 || Loss: 4.0016 || 10iter: 78.7773 sec\n",
      "iteration 990 || Loss: 4.8147 || 10iter: 87.5720 sec\n",
      "iteration 1000 || Loss: 4.1657 || 10iter: 96.5067 sec\n",
      "iteration 1010 || Loss: 5.0415 || 10iter: 105.8584 sec\n",
      "iteration 1020 || Loss: 4.4959 || 10iter: 114.3161 sec\n",
      "iteration 1030 || Loss: 4.2921 || 10iter: 123.4905 sec\n",
      "iteration 1040 || Loss: 4.5108 || 10iter: 132.9342 sec\n",
      "iteration 1050 || Loss: 4.0366 || 10iter: 142.0760 sec\n",
      "iteration 1060 || Loss: 4.0011 || 10iter: 151.5253 sec\n",
      "iteration 1070 || Loss: 4.5360 || 10iter: 161.2734 sec\n",
      "------------\n",
      "epoch 6 || Epoch_TRAIN_Loss: 786.9302 || Epoch_VAL_Loss: 0.0000\n",
      "timer: 164.8705sec\n",
      "------------\n",
      "Epoch 7/50\n",
      "------------\n",
      " (train) \n",
      "iteration 1080 || Loss: 4.4498 || 10iter: 5.1793 sec\n",
      "iteration 1090 || Loss: 4.3085 || 10iter: 14.9192 sec\n",
      "iteration 1100 || Loss: 4.3802 || 10iter: 23.9013 sec\n",
      "iteration 1110 || Loss: 4.2214 || 10iter: 33.2300 sec\n",
      "iteration 1120 || Loss: 4.4321 || 10iter: 42.8390 sec\n",
      "iteration 1130 || Loss: 4.4858 || 10iter: 52.1365 sec\n",
      "iteration 1140 || Loss: 4.0261 || 10iter: 61.0583 sec\n",
      "iteration 1150 || Loss: 3.9042 || 10iter: 70.0730 sec\n",
      "iteration 1160 || Loss: 4.5351 || 10iter: 78.9835 sec\n",
      "iteration 1170 || Loss: 4.7777 || 10iter: 88.1449 sec\n",
      "iteration 1180 || Loss: 3.7877 || 10iter: 97.4768 sec\n",
      "iteration 1190 || Loss: 4.5163 || 10iter: 105.8805 sec\n",
      "iteration 1200 || Loss: 4.3686 || 10iter: 114.6831 sec\n",
      "iteration 1210 || Loss: 4.5842 || 10iter: 124.9003 sec\n",
      "iteration 1220 || Loss: 4.0075 || 10iter: 133.7600 sec\n",
      "iteration 1230 || Loss: 4.4613 || 10iter: 142.8845 sec\n",
      "iteration 1240 || Loss: 4.7497 || 10iter: 151.8166 sec\n",
      "iteration 1250 || Loss: 3.9505 || 10iter: 161.2794 sec\n",
      "------------\n",
      "epoch 7 || Epoch_TRAIN_Loss: 780.8936 || Epoch_VAL_Loss: 0.0000\n",
      "timer: 164.0817sec\n",
      "------------\n",
      "Epoch 8/50\n",
      "------------\n",
      " (train) \n",
      "iteration 1260 || Loss: 4.7389 || 10iter: 5.9788 sec\n",
      "iteration 1270 || Loss: 4.2330 || 10iter: 14.8145 sec\n",
      "iteration 1280 || Loss: 3.7779 || 10iter: 23.7947 sec\n",
      "iteration 1290 || Loss: 4.5312 || 10iter: 32.6809 sec\n",
      "iteration 1300 || Loss: 4.4507 || 10iter: 41.4173 sec\n",
      "iteration 1310 || Loss: 4.6353 || 10iter: 51.2596 sec\n",
      "iteration 1320 || Loss: 4.0411 || 10iter: 59.7153 sec\n",
      "iteration 1330 || Loss: 3.9039 || 10iter: 68.9584 sec\n",
      "iteration 1340 || Loss: 4.2292 || 10iter: 78.7051 sec\n",
      "iteration 1350 || Loss: 4.6910 || 10iter: 87.3887 sec\n",
      "iteration 1360 || Loss: 4.5622 || 10iter: 97.1969 sec\n",
      "iteration 1370 || Loss: 4.4320 || 10iter: 107.4425 sec\n",
      "iteration 1380 || Loss: 3.8029 || 10iter: 116.6378 sec\n",
      "iteration 1390 || Loss: 4.0394 || 10iter: 125.5421 sec\n",
      "iteration 1400 || Loss: 5.0326 || 10iter: 134.8859 sec\n",
      "iteration 1410 || Loss: 4.3576 || 10iter: 144.4699 sec\n",
      "iteration 1420 || Loss: 4.5629 || 10iter: 153.5203 sec\n",
      "iteration 1430 || Loss: 4.7637 || 10iter: 162.3881 sec\n",
      "------------\n",
      "epoch 8 || Epoch_TRAIN_Loss: 776.9314 || Epoch_VAL_Loss: 0.0000\n",
      "timer: 164.2547sec\n",
      "------------\n",
      "Epoch 9/50\n",
      "------------\n",
      " (train) \n",
      "iteration 1440 || Loss: 4.8426 || 10iter: 7.3631 sec\n",
      "iteration 1450 || Loss: 4.8612 || 10iter: 16.2778 sec\n",
      "iteration 1460 || Loss: 4.4540 || 10iter: 24.8639 sec\n",
      "iteration 1470 || Loss: 3.9888 || 10iter: 33.8274 sec\n",
      "iteration 1480 || Loss: 3.8090 || 10iter: 43.9015 sec\n",
      "iteration 1490 || Loss: 4.4912 || 10iter: 53.8739 sec\n",
      "iteration 1500 || Loss: 4.3488 || 10iter: 62.8915 sec\n",
      "iteration 1510 || Loss: 4.8060 || 10iter: 72.1102 sec\n",
      "iteration 1520 || Loss: 4.1613 || 10iter: 80.8661 sec\n",
      "iteration 1530 || Loss: 4.7254 || 10iter: 90.2695 sec\n",
      "iteration 1540 || Loss: 4.5228 || 10iter: 99.4148 sec\n",
      "iteration 1550 || Loss: 4.7800 || 10iter: 108.8141 sec\n",
      "iteration 1560 || Loss: 4.2506 || 10iter: 118.3337 sec\n",
      "iteration 1570 || Loss: 4.5302 || 10iter: 127.7082 sec\n",
      "iteration 1580 || Loss: 3.8691 || 10iter: 136.5511 sec\n",
      "iteration 1590 || Loss: 4.3687 || 10iter: 145.5782 sec\n",
      "iteration 1600 || Loss: 4.1965 || 10iter: 155.3031 sec\n",
      "iteration 1610 || Loss: 4.1824 || 10iter: 164.6777 sec\n",
      "------------\n",
      "epoch 9 || Epoch_TRAIN_Loss: 770.1419 || Epoch_VAL_Loss: 0.0000\n",
      "timer: 165.4459sec\n",
      "------------\n",
      "Epoch 10/50\n",
      "------------\n",
      " (train) \n",
      "iteration 1620 || Loss: 3.8303 || 10iter: 8.0727 sec\n",
      "iteration 1630 || Loss: 4.6013 || 10iter: 17.5394 sec\n",
      "iteration 1640 || Loss: 4.5520 || 10iter: 26.2490 sec\n",
      "iteration 1650 || Loss: 3.9363 || 10iter: 35.2132 sec\n",
      "iteration 1660 || Loss: 4.7259 || 10iter: 44.4732 sec\n",
      "iteration 1670 || Loss: 4.0101 || 10iter: 53.4778 sec\n",
      "iteration 1680 || Loss: 4.2408 || 10iter: 61.9730 sec\n",
      "iteration 1690 || Loss: 3.9427 || 10iter: 70.5755 sec\n",
      "iteration 1700 || Loss: 4.5693 || 10iter: 80.9372 sec\n",
      "iteration 1710 || Loss: 4.3315 || 10iter: 90.5608 sec\n",
      "iteration 1720 || Loss: 4.2901 || 10iter: 99.6900 sec\n",
      "iteration 1730 || Loss: 3.9554 || 10iter: 109.1175 sec\n",
      "iteration 1740 || Loss: 4.5279 || 10iter: 118.8072 sec\n",
      "iteration 1750 || Loss: 4.2564 || 10iter: 127.5756 sec\n",
      "iteration 1760 || Loss: 4.4931 || 10iter: 137.1243 sec\n",
      "iteration 1770 || Loss: 4.3452 || 10iter: 145.7283 sec\n",
      "iteration 1780 || Loss: 4.8262 || 10iter: 155.0194 sec\n",
      "iteration 1790 || Loss: 3.7294 || 10iter: 163.5349 sec\n",
      " (val) \n",
      "------------\n",
      "epoch 10 || Epoch_TRAIN_Loss: 759.1504 || Epoch_VAL_Loss: 785.0407\n",
      "timer: 226.4924sec\n",
      "------------\n",
      "Epoch 11/50\n",
      "------------\n",
      " (train) \n",
      "iteration 1800 || Loss: 4.3430 || 10iter: 9.3446 sec\n",
      "iteration 1810 || Loss: 4.0406 || 10iter: 17.9555 sec\n",
      "iteration 1820 || Loss: 4.6123 || 10iter: 27.0397 sec\n",
      "iteration 1830 || Loss: 4.2285 || 10iter: 35.7585 sec\n",
      "iteration 1840 || Loss: 4.0158 || 10iter: 44.8192 sec\n",
      "iteration 1850 || Loss: 3.1688 || 10iter: 53.7088 sec\n",
      "iteration 1860 || Loss: 3.9653 || 10iter: 62.6976 sec\n",
      "iteration 1870 || Loss: 4.0702 || 10iter: 71.5304 sec\n",
      "iteration 1880 || Loss: 4.5009 || 10iter: 80.7795 sec\n",
      "iteration 1890 || Loss: 4.6926 || 10iter: 89.7202 sec\n",
      "iteration 1900 || Loss: 3.5287 || 10iter: 98.6896 sec\n",
      "iteration 1910 || Loss: 3.7248 || 10iter: 107.3878 sec\n",
      "iteration 1920 || Loss: 4.1051 || 10iter: 116.0421 sec\n",
      "iteration 1930 || Loss: 4.0318 || 10iter: 124.9133 sec\n",
      "iteration 1940 || Loss: 4.5250 || 10iter: 133.7005 sec\n",
      "iteration 1950 || Loss: 4.1299 || 10iter: 142.2084 sec\n",
      "iteration 1960 || Loss: 3.9614 || 10iter: 151.1756 sec\n",
      "------------\n",
      "epoch 11 || Epoch_TRAIN_Loss: 749.1934 || Epoch_VAL_Loss: 0.0000\n",
      "timer: 159.3201sec\n",
      "------------\n",
      "Epoch 12/50\n",
      "------------\n",
      " (train) \n",
      "iteration 1970 || Loss: 3.7817 || 10iter: 0.8451 sec\n",
      "iteration 1980 || Loss: 4.5352 || 10iter: 10.0223 sec\n",
      "iteration 1990 || Loss: 4.0963 || 10iter: 18.9026 sec\n",
      "iteration 2000 || Loss: 3.9651 || 10iter: 27.5592 sec\n",
      "iteration 2010 || Loss: 3.8171 || 10iter: 36.9476 sec\n",
      "iteration 2020 || Loss: 4.2229 || 10iter: 46.1464 sec\n",
      "iteration 2030 || Loss: 4.5925 || 10iter: 54.9700 sec\n",
      "iteration 2040 || Loss: 3.8869 || 10iter: 63.3789 sec\n",
      "iteration 2050 || Loss: 4.0833 || 10iter: 72.3419 sec\n",
      "iteration 2060 || Loss: 4.4375 || 10iter: 81.4435 sec\n",
      "iteration 2070 || Loss: 3.8532 || 10iter: 90.3658 sec\n",
      "iteration 2080 || Loss: 3.4832 || 10iter: 99.5249 sec\n",
      "iteration 2090 || Loss: 3.9428 || 10iter: 108.9250 sec\n",
      "iteration 2100 || Loss: 3.7059 || 10iter: 118.0818 sec\n",
      "iteration 2110 || Loss: 4.0190 || 10iter: 126.6970 sec\n",
      "iteration 2120 || Loss: 4.6005 || 10iter: 135.8740 sec\n",
      "iteration 2130 || Loss: 3.9195 || 10iter: 145.2238 sec\n",
      "iteration 2140 || Loss: 3.8893 || 10iter: 154.0812 sec\n",
      "------------\n",
      "epoch 12 || Epoch_TRAIN_Loss: 744.2298 || Epoch_VAL_Loss: 0.0000\n",
      "timer: 161.7744sec\n",
      "------------\n",
      "Epoch 13/50\n",
      "------------\n",
      " (train) \n",
      "iteration 2150 || Loss: 3.6461 || 10iter: 1.5457 sec\n",
      "iteration 2160 || Loss: 4.0190 || 10iter: 10.2797 sec\n",
      "iteration 2170 || Loss: 3.9557 || 10iter: 19.1802 sec\n",
      "iteration 2180 || Loss: 4.0454 || 10iter: 28.1314 sec\n",
      "iteration 2190 || Loss: 4.3339 || 10iter: 37.1528 sec\n",
      "iteration 2200 || Loss: 4.2756 || 10iter: 45.8965 sec\n",
      "iteration 2210 || Loss: 3.7778 || 10iter: 54.5156 sec\n",
      "iteration 2220 || Loss: 4.7100 || 10iter: 63.1255 sec\n",
      "iteration 2230 || Loss: 4.1350 || 10iter: 71.9143 sec\n",
      "iteration 2240 || Loss: 3.9843 || 10iter: 81.6098 sec\n",
      "iteration 2250 || Loss: 4.0612 || 10iter: 90.5248 sec\n",
      "iteration 2260 || Loss: 4.2417 || 10iter: 99.9757 sec\n",
      "iteration 2270 || Loss: 4.1682 || 10iter: 109.2464 sec\n",
      "iteration 2280 || Loss: 3.9488 || 10iter: 118.5641 sec\n",
      "iteration 2290 || Loss: 4.0473 || 10iter: 127.7614 sec\n",
      "iteration 2300 || Loss: 4.3723 || 10iter: 137.5617 sec\n",
      "iteration 2310 || Loss: 3.8363 || 10iter: 146.1316 sec\n",
      "iteration 2320 || Loss: 4.0554 || 10iter: 155.3354 sec\n",
      "------------\n",
      "epoch 13 || Epoch_TRAIN_Loss: 741.3967 || Epoch_VAL_Loss: 0.0000\n",
      "timer: 161.3822sec\n",
      "------------\n",
      "Epoch 14/50\n",
      "------------\n",
      " (train) \n",
      "iteration 2330 || Loss: 4.5778 || 10iter: 2.5167 sec\n",
      "iteration 2340 || Loss: 3.7442 || 10iter: 11.1183 sec\n",
      "iteration 2350 || Loss: 4.2419 || 10iter: 20.2353 sec\n",
      "iteration 2360 || Loss: 3.5961 || 10iter: 29.6530 sec\n",
      "iteration 2370 || Loss: 4.0849 || 10iter: 38.0608 sec\n",
      "iteration 2380 || Loss: 4.0285 || 10iter: 47.1976 sec\n",
      "iteration 2390 || Loss: 4.6189 || 10iter: 56.4403 sec\n",
      "iteration 2400 || Loss: 4.2329 || 10iter: 64.9790 sec\n",
      "iteration 2410 || Loss: 4.7201 || 10iter: 73.8685 sec\n",
      "iteration 2420 || Loss: 4.3309 || 10iter: 83.8401 sec\n",
      "iteration 2430 || Loss: 4.0338 || 10iter: 92.8758 sec\n",
      "iteration 2440 || Loss: 4.3625 || 10iter: 102.1675 sec\n",
      "iteration 2450 || Loss: 3.8171 || 10iter: 111.8188 sec\n",
      "iteration 2460 || Loss: 4.5246 || 10iter: 121.4112 sec\n",
      "iteration 2470 || Loss: 3.9257 || 10iter: 130.1058 sec\n",
      "iteration 2480 || Loss: 4.1868 || 10iter: 139.1153 sec\n",
      "iteration 2490 || Loss: 3.9016 || 10iter: 148.7311 sec\n",
      "iteration 2500 || Loss: 3.8634 || 10iter: 157.2765 sec\n",
      "------------\n",
      "epoch 14 || Epoch_TRAIN_Loss: 735.9181 || Epoch_VAL_Loss: 0.0000\n",
      "timer: 162.5457sec\n",
      "------------\n",
      "Epoch 15/50\n",
      "------------\n",
      " (train) \n",
      "iteration 2510 || Loss: 3.6563 || 10iter: 3.6858 sec\n",
      "iteration 2520 || Loss: 4.1644 || 10iter: 12.7051 sec\n",
      "iteration 2530 || Loss: 4.0691 || 10iter: 22.0359 sec\n",
      "iteration 2540 || Loss: 4.1753 || 10iter: 31.0383 sec\n",
      "iteration 2550 || Loss: 4.1845 || 10iter: 39.2627 sec\n",
      "iteration 2560 || Loss: 3.5449 || 10iter: 49.1162 sec\n",
      "iteration 2570 || Loss: 4.5395 || 10iter: 58.5776 sec\n",
      "iteration 2580 || Loss: 3.6746 || 10iter: 67.1965 sec\n",
      "iteration 2590 || Loss: 4.4287 || 10iter: 76.7987 sec\n",
      "iteration 2600 || Loss: 3.8424 || 10iter: 86.2311 sec\n",
      "iteration 2610 || Loss: 3.8863 || 10iter: 95.2161 sec\n",
      "iteration 2620 || Loss: 4.6040 || 10iter: 104.5100 sec\n",
      "iteration 2630 || Loss: 3.4097 || 10iter: 113.4927 sec\n",
      "iteration 2640 || Loss: 4.0503 || 10iter: 122.9249 sec\n",
      "iteration 2650 || Loss: 3.3242 || 10iter: 132.4645 sec\n",
      "iteration 2660 || Loss: 4.2355 || 10iter: 142.0490 sec\n",
      "iteration 2670 || Loss: 4.0996 || 10iter: 150.7084 sec\n",
      "iteration 2680 || Loss: 4.1949 || 10iter: 159.7240 sec\n",
      "------------\n",
      "epoch 15 || Epoch_TRAIN_Loss: 725.1982 || Epoch_VAL_Loss: 0.0000\n",
      "timer: 164.3417sec\n",
      "------------\n",
      "Epoch 16/50\n",
      "------------\n",
      " (train) \n",
      "iteration 2690 || Loss: 4.3504 || 10iter: 4.6120 sec\n",
      "iteration 2700 || Loss: 4.1047 || 10iter: 13.7163 sec\n",
      "iteration 2710 || Loss: 3.6511 || 10iter: 22.8768 sec\n",
      "iteration 2720 || Loss: 4.1312 || 10iter: 31.5608 sec\n",
      "iteration 2730 || Loss: 3.4000 || 10iter: 40.6666 sec\n",
      "iteration 2740 || Loss: 3.8462 || 10iter: 49.9034 sec\n",
      "iteration 2750 || Loss: 4.0223 || 10iter: 58.5961 sec\n",
      "iteration 2760 || Loss: 4.3036 || 10iter: 67.8821 sec\n",
      "iteration 2770 || Loss: 3.4482 || 10iter: 76.3182 sec\n",
      "iteration 2780 || Loss: 4.4428 || 10iter: 84.9494 sec\n",
      "iteration 2790 || Loss: 3.8672 || 10iter: 93.7447 sec\n",
      "iteration 2800 || Loss: 4.5027 || 10iter: 102.7938 sec\n",
      "iteration 2810 || Loss: 4.4029 || 10iter: 111.5008 sec\n",
      "iteration 2820 || Loss: 3.9664 || 10iter: 120.1877 sec\n",
      "iteration 2830 || Loss: 3.9334 || 10iter: 129.5626 sec\n",
      "iteration 2840 || Loss: 3.8741 || 10iter: 138.6232 sec\n",
      "iteration 2850 || Loss: 3.9918 || 10iter: 148.3305 sec\n",
      "iteration 2860 || Loss: 4.8159 || 10iter: 157.8313 sec\n",
      "------------\n",
      "epoch 16 || Epoch_TRAIN_Loss: 722.2917 || Epoch_VAL_Loss: 0.0000\n",
      "timer: 161.2954sec\n",
      "------------\n",
      "Epoch 17/50\n",
      "------------\n",
      " (train) \n",
      "iteration 2870 || Loss: 4.0111 || 10iter: 5.3510 sec\n",
      "iteration 2880 || Loss: 4.2775 || 10iter: 14.8357 sec\n",
      "iteration 2890 || Loss: 4.0957 || 10iter: 24.1364 sec\n",
      "iteration 2900 || Loss: 3.5426 || 10iter: 32.8814 sec\n",
      "iteration 2910 || Loss: 3.6133 || 10iter: 41.6675 sec\n",
      "iteration 2920 || Loss: 3.9507 || 10iter: 50.5235 sec\n",
      "iteration 2930 || Loss: 4.0020 || 10iter: 59.7172 sec\n",
      "iteration 2940 || Loss: 3.9546 || 10iter: 69.1219 sec\n",
      "iteration 2950 || Loss: 3.6554 || 10iter: 78.2687 sec\n",
      "iteration 2960 || Loss: 4.3409 || 10iter: 87.0723 sec\n",
      "iteration 2970 || Loss: 3.6369 || 10iter: 96.5148 sec\n",
      "iteration 2980 || Loss: 3.8762 || 10iter: 105.4193 sec\n",
      "iteration 2990 || Loss: 4.2370 || 10iter: 115.0681 sec\n",
      "iteration 3000 || Loss: 4.1269 || 10iter: 124.8021 sec\n",
      "iteration 3010 || Loss: 4.1260 || 10iter: 134.0712 sec\n",
      "iteration 3020 || Loss: 4.1354 || 10iter: 142.5949 sec\n",
      "iteration 3030 || Loss: 4.3045 || 10iter: 151.7000 sec\n",
      "iteration 3040 || Loss: 4.0252 || 10iter: 160.4803 sec\n",
      "------------\n",
      "epoch 17 || Epoch_TRAIN_Loss: 713.4910 || Epoch_VAL_Loss: 0.0000\n",
      "timer: 162.9827sec\n",
      "------------\n",
      "Epoch 18/50\n",
      "------------\n",
      " (train) \n",
      "iteration 3050 || Loss: 3.7809 || 10iter: 6.0551 sec\n",
      "iteration 3060 || Loss: 4.4479 || 10iter: 15.4918 sec\n",
      "iteration 3070 || Loss: 3.2409 || 10iter: 23.9795 sec\n",
      "iteration 3080 || Loss: 3.5071 || 10iter: 32.7481 sec\n",
      "iteration 3090 || Loss: 4.2035 || 10iter: 42.3226 sec\n",
      "iteration 3100 || Loss: 3.6663 || 10iter: 51.2179 sec\n",
      "iteration 3110 || Loss: 4.3930 || 10iter: 60.3074 sec\n",
      "iteration 3120 || Loss: 4.0294 || 10iter: 69.4099 sec\n",
      "iteration 3130 || Loss: 3.8150 || 10iter: 79.6414 sec\n",
      "iteration 3140 || Loss: 3.7271 || 10iter: 89.3104 sec\n",
      "iteration 3150 || Loss: 3.8449 || 10iter: 98.9296 sec\n",
      "iteration 3160 || Loss: 4.2323 || 10iter: 108.2237 sec\n",
      "iteration 3170 || Loss: 3.9902 || 10iter: 117.1365 sec\n",
      "iteration 3180 || Loss: 4.3287 || 10iter: 126.5341 sec\n",
      "iteration 3190 || Loss: 4.0327 || 10iter: 135.7806 sec\n",
      "iteration 3200 || Loss: 3.9786 || 10iter: 144.5762 sec\n",
      "iteration 3210 || Loss: 3.8007 || 10iter: 153.9000 sec\n",
      "iteration 3220 || Loss: 4.0610 || 10iter: 163.0034 sec\n",
      "------------\n",
      "epoch 18 || Epoch_TRAIN_Loss: 708.1549 || Epoch_VAL_Loss: 0.0000\n",
      "timer: 165.1672sec\n",
      "------------\n",
      "Epoch 19/50\n",
      "------------\n",
      " (train) \n",
      "iteration 3230 || Loss: 3.2819 || 10iter: 6.7898 sec\n",
      "iteration 3240 || Loss: 4.4858 || 10iter: 16.5048 sec\n",
      "iteration 3250 || Loss: 4.1317 || 10iter: 25.9773 sec\n",
      "iteration 3260 || Loss: 3.8718 || 10iter: 35.0573 sec\n",
      "iteration 3270 || Loss: 3.0344 || 10iter: 43.6264 sec\n",
      "iteration 3280 || Loss: 3.7877 || 10iter: 53.0315 sec\n",
      "iteration 3290 || Loss: 3.9054 || 10iter: 61.7368 sec\n",
      "iteration 3300 || Loss: 4.2907 || 10iter: 70.1836 sec\n",
      "iteration 3310 || Loss: 3.5234 || 10iter: 79.2363 sec\n",
      "iteration 3320 || Loss: 3.7610 || 10iter: 87.9993 sec\n",
      "iteration 3330 || Loss: 3.8437 || 10iter: 97.6507 sec\n",
      "iteration 3340 || Loss: 3.9125 || 10iter: 106.3063 sec\n",
      "iteration 3350 || Loss: 3.8034 || 10iter: 115.9537 sec\n",
      "iteration 3360 || Loss: 4.3977 || 10iter: 125.1341 sec\n",
      "iteration 3370 || Loss: 3.7038 || 10iter: 134.0965 sec\n",
      "iteration 3380 || Loss: 3.6693 || 10iter: 143.5661 sec\n",
      "iteration 3390 || Loss: 3.2437 || 10iter: 152.8680 sec\n",
      "iteration 3400 || Loss: 3.8472 || 10iter: 162.1271 sec\n",
      "------------\n",
      "epoch 19 || Epoch_TRAIN_Loss: 702.7210 || Epoch_VAL_Loss: 0.0000\n",
      "timer: 163.1656sec\n",
      "------------\n",
      "Epoch 20/50\n",
      "------------\n",
      " (train) \n",
      "iteration 3410 || Loss: 3.9580 || 10iter: 7.9262 sec\n",
      "iteration 3420 || Loss: 3.8435 || 10iter: 16.1184 sec\n",
      "iteration 3430 || Loss: 3.7265 || 10iter: 25.1210 sec\n",
      "iteration 3440 || Loss: 3.6838 || 10iter: 33.8879 sec\n",
      "iteration 3450 || Loss: 4.2888 || 10iter: 43.8169 sec\n",
      "iteration 3460 || Loss: 3.8117 || 10iter: 53.0872 sec\n",
      "iteration 3470 || Loss: 4.2509 || 10iter: 62.4186 sec\n",
      "iteration 3480 || Loss: 3.7881 || 10iter: 71.6123 sec\n",
      "iteration 3490 || Loss: 3.8943 || 10iter: 81.1957 sec\n",
      "iteration 3500 || Loss: 3.9169 || 10iter: 89.7991 sec\n",
      "iteration 3510 || Loss: 3.9094 || 10iter: 98.8261 sec\n",
      "iteration 3520 || Loss: 3.6519 || 10iter: 108.1310 sec\n",
      "iteration 3530 || Loss: 3.8054 || 10iter: 116.5158 sec\n",
      "iteration 3540 || Loss: 4.0393 || 10iter: 125.0418 sec\n",
      "iteration 3550 || Loss: 3.5822 || 10iter: 133.6025 sec\n",
      "iteration 3560 || Loss: 3.5532 || 10iter: 142.9409 sec\n",
      "iteration 3570 || Loss: 3.5821 || 10iter: 152.1692 sec\n",
      "iteration 3580 || Loss: 4.1739 || 10iter: 160.7320 sec\n",
      " (val) \n",
      "------------\n",
      "epoch 20 || Epoch_TRAIN_Loss: 691.7025 || Epoch_VAL_Loss: 742.3303\n",
      "timer: 223.8193sec\n",
      "------------\n",
      "Epoch 21/50\n",
      "------------\n",
      " (train) \n",
      "iteration 3590 || Loss: 3.8612 || 10iter: 8.6501 sec\n",
      "iteration 3600 || Loss: 3.8415 || 10iter: 18.4005 sec\n",
      "iteration 3610 || Loss: 3.5873 || 10iter: 27.0268 sec\n",
      "iteration 3620 || Loss: 3.7164 || 10iter: 36.4439 sec\n",
      "iteration 3630 || Loss: 3.6901 || 10iter: 45.2582 sec\n",
      "iteration 3640 || Loss: 4.1874 || 10iter: 54.1232 sec\n",
      "iteration 3650 || Loss: 3.7866 || 10iter: 63.9273 sec\n",
      "iteration 3660 || Loss: 3.7563 || 10iter: 72.6484 sec\n",
      "iteration 3670 || Loss: 3.3859 || 10iter: 81.5546 sec\n",
      "iteration 3680 || Loss: 3.1525 || 10iter: 90.5945 sec\n",
      "iteration 3690 || Loss: 3.6705 || 10iter: 99.4428 sec\n",
      "iteration 3700 || Loss: 3.8999 || 10iter: 108.2811 sec\n",
      "iteration 3710 || Loss: 3.5632 || 10iter: 117.0420 sec\n",
      "iteration 3720 || Loss: 3.8022 || 10iter: 125.6975 sec\n",
      "iteration 3730 || Loss: 3.5348 || 10iter: 135.2729 sec\n",
      "iteration 3740 || Loss: 3.3439 || 10iter: 144.3221 sec\n",
      "iteration 3750 || Loss: 4.0786 || 10iter: 153.2128 sec\n",
      "------------\n",
      "epoch 21 || Epoch_TRAIN_Loss: 687.5203 || Epoch_VAL_Loss: 0.0000\n",
      "timer: 161.3456sec\n",
      "------------\n",
      "Epoch 22/50\n",
      "------------\n",
      " (train) \n",
      "iteration 3760 || Loss: 3.4359 || 10iter: 0.6190 sec\n",
      "iteration 3770 || Loss: 3.5868 || 10iter: 9.4500 sec\n",
      "iteration 3780 || Loss: 3.6274 || 10iter: 18.3247 sec\n",
      "iteration 3790 || Loss: 3.9445 || 10iter: 28.0880 sec\n",
      "iteration 3800 || Loss: 3.9240 || 10iter: 37.0872 sec\n",
      "iteration 3810 || Loss: 3.7841 || 10iter: 46.4364 sec\n",
      "iteration 3820 || Loss: 3.8009 || 10iter: 55.7211 sec\n",
      "iteration 3830 || Loss: 4.2922 || 10iter: 64.7273 sec\n",
      "iteration 3840 || Loss: 3.4327 || 10iter: 73.7170 sec\n",
      "iteration 3850 || Loss: 4.2933 || 10iter: 82.5274 sec\n",
      "iteration 3860 || Loss: 3.7256 || 10iter: 91.8924 sec\n",
      "iteration 3870 || Loss: 4.5325 || 10iter: 100.8801 sec\n",
      "iteration 3880 || Loss: 3.9403 || 10iter: 110.0136 sec\n",
      "iteration 3890 || Loss: 3.7267 || 10iter: 118.6316 sec\n",
      "iteration 3900 || Loss: 3.7040 || 10iter: 127.9093 sec\n",
      "iteration 3910 || Loss: 3.5550 || 10iter: 137.7532 sec\n",
      "iteration 3920 || Loss: 4.0169 || 10iter: 146.6839 sec\n",
      "iteration 3930 || Loss: 3.8347 || 10iter: 155.4360 sec\n",
      "------------\n",
      "epoch 22 || Epoch_TRAIN_Loss: 686.6254 || Epoch_VAL_Loss: 0.0000\n",
      "timer: 163.2419sec\n",
      "------------\n",
      "Epoch 23/50\n",
      "------------\n",
      " (train) \n",
      "iteration 3940 || Loss: 3.9582 || 10iter: 1.9113 sec\n",
      "iteration 3950 || Loss: 3.8683 || 10iter: 10.5994 sec\n",
      "iteration 3960 || Loss: 3.6841 || 10iter: 19.4849 sec\n",
      "iteration 3970 || Loss: 3.6388 || 10iter: 28.5375 sec\n",
      "iteration 3980 || Loss: 3.4428 || 10iter: 37.0637 sec\n",
      "iteration 3990 || Loss: 3.2018 || 10iter: 46.2310 sec\n",
      "iteration 4000 || Loss: 3.7080 || 10iter: 55.2890 sec\n",
      "iteration 4010 || Loss: 3.7550 || 10iter: 64.8953 sec\n",
      "iteration 4020 || Loss: 4.4245 || 10iter: 73.7102 sec\n",
      "iteration 4030 || Loss: 3.8031 || 10iter: 82.3480 sec\n",
      "iteration 4040 || Loss: 2.9873 || 10iter: 91.5610 sec\n",
      "iteration 4050 || Loss: 4.4889 || 10iter: 100.6511 sec\n",
      "iteration 4060 || Loss: 4.5666 || 10iter: 109.9871 sec\n",
      "iteration 4070 || Loss: 3.2486 || 10iter: 119.5065 sec\n",
      "iteration 4080 || Loss: 3.5951 || 10iter: 129.0279 sec\n",
      "iteration 4090 || Loss: 3.9011 || 10iter: 138.0586 sec\n",
      "iteration 4100 || Loss: 3.7963 || 10iter: 147.0339 sec\n",
      "iteration 4110 || Loss: 4.0300 || 10iter: 157.2069 sec\n",
      "------------\n",
      "epoch 23 || Epoch_TRAIN_Loss: 679.5388 || Epoch_VAL_Loss: 0.0000\n",
      "timer: 163.4625sec\n",
      "------------\n",
      "Epoch 24/50\n",
      "------------\n",
      " (train) \n",
      "iteration 4120 || Loss: 3.6904 || 10iter: 2.4696 sec\n",
      "iteration 4130 || Loss: 3.4341 || 10iter: 11.4046 sec\n",
      "iteration 4140 || Loss: 3.6214 || 10iter: 20.7322 sec\n",
      "iteration 4150 || Loss: 3.8272 || 10iter: 30.0443 sec\n",
      "iteration 4160 || Loss: 4.0042 || 10iter: 38.9598 sec\n",
      "iteration 4170 || Loss: 3.7725 || 10iter: 48.8200 sec\n",
      "iteration 4180 || Loss: 3.3832 || 10iter: 58.3111 sec\n",
      "iteration 4190 || Loss: 3.8166 || 10iter: 67.8192 sec\n",
      "iteration 4200 || Loss: 3.6811 || 10iter: 77.5759 sec\n",
      "iteration 4210 || Loss: 3.9543 || 10iter: 86.4738 sec\n",
      "iteration 4220 || Loss: 3.5025 || 10iter: 95.4856 sec\n",
      "iteration 4230 || Loss: 4.2734 || 10iter: 104.5037 sec\n",
      "iteration 4240 || Loss: 2.7769 || 10iter: 115.1351 sec\n",
      "iteration 4250 || Loss: 3.7178 || 10iter: 124.3835 sec\n",
      "iteration 4260 || Loss: 3.5142 || 10iter: 133.4981 sec\n",
      "iteration 4270 || Loss: 3.8286 || 10iter: 141.8041 sec\n",
      "iteration 4280 || Loss: 4.1283 || 10iter: 151.4302 sec\n",
      "iteration 4290 || Loss: 3.6931 || 10iter: 160.7837 sec\n",
      "------------\n",
      "epoch 24 || Epoch_TRAIN_Loss: 675.2902 || Epoch_VAL_Loss: 0.0000\n",
      "timer: 166.4056sec\n",
      "------------\n",
      "Epoch 25/50\n",
      "------------\n",
      " (train) \n",
      "iteration 4300 || Loss: 3.5581 || 10iter: 3.5130 sec\n",
      "iteration 4310 || Loss: 4.1585 || 10iter: 12.4173 sec\n",
      "iteration 4320 || Loss: 3.8540 || 10iter: 22.2178 sec\n",
      "iteration 4330 || Loss: 3.7469 || 10iter: 31.5438 sec\n",
      "iteration 4340 || Loss: 4.4300 || 10iter: 40.7934 sec\n",
      "iteration 4350 || Loss: 3.7948 || 10iter: 50.1554 sec\n",
      "iteration 4360 || Loss: 3.7151 || 10iter: 59.8893 sec\n",
      "iteration 4370 || Loss: 3.4231 || 10iter: 68.8570 sec\n",
      "iteration 4380 || Loss: 3.9672 || 10iter: 78.3528 sec\n",
      "iteration 4390 || Loss: 3.8846 || 10iter: 86.9908 sec\n",
      "iteration 4400 || Loss: 3.7196 || 10iter: 96.4016 sec\n",
      "iteration 4410 || Loss: 3.4383 || 10iter: 106.0035 sec\n",
      "iteration 4420 || Loss: 3.5450 || 10iter: 115.0526 sec\n",
      "iteration 4430 || Loss: 3.8276 || 10iter: 124.1597 sec\n",
      "iteration 4440 || Loss: 3.7212 || 10iter: 134.1313 sec\n",
      "iteration 4450 || Loss: 3.6278 || 10iter: 142.9346 sec\n",
      "iteration 4460 || Loss: 3.4462 || 10iter: 152.1952 sec\n",
      "iteration 4470 || Loss: 3.9459 || 10iter: 161.3905 sec\n",
      "------------\n",
      "epoch 25 || Epoch_TRAIN_Loss: 674.5536 || Epoch_VAL_Loss: 0.0000\n",
      "timer: 165.6026sec\n",
      "------------\n",
      "Epoch 26/50\n",
      "------------\n",
      " (train) \n",
      "iteration 4480 || Loss: 3.6948 || 10iter: 4.2475 sec\n",
      "iteration 4490 || Loss: 3.8843 || 10iter: 14.0747 sec\n",
      "iteration 4500 || Loss: 3.1626 || 10iter: 23.7262 sec\n",
      "iteration 4510 || Loss: 4.1660 || 10iter: 32.6388 sec\n",
      "iteration 4520 || Loss: 3.5072 || 10iter: 41.7364 sec\n",
      "iteration 4530 || Loss: 3.5808 || 10iter: 50.8599 sec\n",
      "iteration 4540 || Loss: 3.7312 || 10iter: 60.6246 sec\n",
      "iteration 4550 || Loss: 3.7152 || 10iter: 70.1114 sec\n",
      "iteration 4560 || Loss: 3.3898 || 10iter: 79.0731 sec\n",
      "iteration 4570 || Loss: 3.9318 || 10iter: 87.7414 sec\n",
      "iteration 4580 || Loss: 3.9352 || 10iter: 96.6157 sec\n",
      "iteration 4590 || Loss: 3.7135 || 10iter: 105.2706 sec\n",
      "iteration 4600 || Loss: 3.2896 || 10iter: 113.7012 sec\n",
      "iteration 4610 || Loss: 3.5962 || 10iter: 122.3006 sec\n",
      "iteration 4620 || Loss: 3.7239 || 10iter: 132.5188 sec\n",
      "iteration 4630 || Loss: 3.5393 || 10iter: 142.3212 sec\n",
      "iteration 4640 || Loss: 3.1867 || 10iter: 151.8062 sec\n",
      "iteration 4650 || Loss: 3.5530 || 10iter: 160.8947 sec\n",
      "------------\n",
      "epoch 26 || Epoch_TRAIN_Loss: 663.3562 || Epoch_VAL_Loss: 0.0000\n",
      "timer: 164.2858sec\n",
      "------------\n",
      "Epoch 27/50\n",
      "------------\n",
      " (train) \n",
      "iteration 4660 || Loss: 3.8252 || 10iter: 5.6634 sec\n",
      "iteration 4670 || Loss: 3.3355 || 10iter: 14.3978 sec\n",
      "iteration 4680 || Loss: 3.8570 || 10iter: 24.5443 sec\n",
      "iteration 4690 || Loss: 3.4623 || 10iter: 34.0439 sec\n",
      "iteration 4700 || Loss: 3.3919 || 10iter: 42.9800 sec\n",
      "iteration 4710 || Loss: 3.8543 || 10iter: 51.6985 sec\n",
      "iteration 4720 || Loss: 3.5935 || 10iter: 60.8485 sec\n",
      "iteration 4730 || Loss: 3.6936 || 10iter: 69.7296 sec\n",
      "iteration 4740 || Loss: 3.5680 || 10iter: 78.7017 sec\n",
      "iteration 4750 || Loss: 2.9425 || 10iter: 87.7446 sec\n",
      "iteration 4760 || Loss: 3.7616 || 10iter: 96.4781 sec\n",
      "iteration 4770 || Loss: 3.3191 || 10iter: 106.0681 sec\n",
      "iteration 4780 || Loss: 3.8416 || 10iter: 116.3617 sec\n",
      "iteration 4790 || Loss: 3.4144 || 10iter: 125.8252 sec\n",
      "iteration 4800 || Loss: 4.1496 || 10iter: 135.5463 sec\n",
      "iteration 4810 || Loss: 3.4628 || 10iter: 144.8880 sec\n",
      "iteration 4820 || Loss: 3.9138 || 10iter: 154.6731 sec\n",
      "iteration 4830 || Loss: 3.8281 || 10iter: 163.1221 sec\n",
      "------------\n",
      "epoch 27 || Epoch_TRAIN_Loss: 657.3921 || Epoch_VAL_Loss: 0.0000\n",
      "timer: 165.5876sec\n",
      "------------\n",
      "Epoch 28/50\n",
      "------------\n",
      " (train) \n",
      "iteration 4840 || Loss: 3.6216 || 10iter: 6.0275 sec\n",
      "iteration 4850 || Loss: 3.2080 || 10iter: 15.6947 sec\n",
      "iteration 4860 || Loss: 3.6978 || 10iter: 25.5672 sec\n",
      "iteration 4870 || Loss: 2.8723 || 10iter: 34.4563 sec\n",
      "iteration 4880 || Loss: 3.5697 || 10iter: 44.1486 sec\n",
      "iteration 4890 || Loss: 3.7202 || 10iter: 52.7958 sec\n",
      "iteration 4900 || Loss: 3.5730 || 10iter: 61.4678 sec\n",
      "iteration 4910 || Loss: 4.0400 || 10iter: 70.5347 sec\n",
      "iteration 4920 || Loss: 3.8267 || 10iter: 80.1916 sec\n",
      "iteration 4930 || Loss: 3.2504 || 10iter: 89.4456 sec\n",
      "iteration 4940 || Loss: 3.5590 || 10iter: 98.3705 sec\n",
      "iteration 4950 || Loss: 3.4561 || 10iter: 107.4439 sec\n",
      "iteration 4960 || Loss: 3.7533 || 10iter: 116.5881 sec\n",
      "iteration 4970 || Loss: 3.6694 || 10iter: 125.7013 sec\n",
      "iteration 4980 || Loss: 4.3403 || 10iter: 134.9912 sec\n",
      "iteration 4990 || Loss: 3.6682 || 10iter: 144.1520 sec\n",
      "iteration 5000 || Loss: 3.3645 || 10iter: 152.6382 sec\n",
      "iteration 5010 || Loss: 3.5680 || 10iter: 161.7662 sec\n",
      "------------\n",
      "epoch 28 || Epoch_TRAIN_Loss: 645.9242 || Epoch_VAL_Loss: 0.0000\n",
      "timer: 163.3827sec\n",
      "------------\n",
      "Epoch 29/50\n",
      "------------\n",
      " (train) \n",
      "iteration 5020 || Loss: 3.4122 || 10iter: 6.8767 sec\n",
      "iteration 5030 || Loss: 3.5821 || 10iter: 16.3687 sec\n",
      "iteration 5040 || Loss: 3.6218 || 10iter: 25.9006 sec\n",
      "iteration 5050 || Loss: 3.7377 || 10iter: 34.7739 sec\n",
      "iteration 5060 || Loss: 2.9966 || 10iter: 44.0876 sec\n",
      "iteration 5070 || Loss: 3.3327 || 10iter: 52.8276 sec\n",
      "iteration 5080 || Loss: 3.9186 || 10iter: 62.1763 sec\n",
      "iteration 5090 || Loss: 3.7279 || 10iter: 70.4651 sec\n",
      "iteration 5100 || Loss: 3.3172 || 10iter: 79.9542 sec\n",
      "iteration 5110 || Loss: 3.4544 || 10iter: 88.7997 sec\n",
      "iteration 5120 || Loss: 3.8780 || 10iter: 97.3107 sec\n",
      "iteration 5130 || Loss: 3.5601 || 10iter: 106.2523 sec\n",
      "iteration 5140 || Loss: 3.1099 || 10iter: 115.3866 sec\n",
      "iteration 5150 || Loss: 3.6478 || 10iter: 124.4703 sec\n",
      "iteration 5160 || Loss: 3.6353 || 10iter: 133.8163 sec\n",
      "iteration 5170 || Loss: 3.7722 || 10iter: 142.4682 sec\n",
      "iteration 5180 || Loss: 3.3087 || 10iter: 153.2216 sec\n",
      "iteration 5190 || Loss: 3.6509 || 10iter: 161.8392 sec\n",
      "------------\n",
      "epoch 29 || Epoch_TRAIN_Loss: 642.8158 || Epoch_VAL_Loss: 0.0000\n",
      "timer: 162.6486sec\n",
      "------------\n",
      "Epoch 30/50\n",
      "------------\n",
      " (train) \n",
      "iteration 5200 || Loss: 3.5054 || 10iter: 8.0583 sec\n",
      "iteration 5210 || Loss: 3.2293 || 10iter: 17.5555 sec\n",
      "iteration 5220 || Loss: 3.9629 || 10iter: 26.5696 sec\n",
      "iteration 5230 || Loss: 3.8521 || 10iter: 35.5975 sec\n",
      "iteration 5240 || Loss: 3.3066 || 10iter: 45.1315 sec\n",
      "iteration 5250 || Loss: 4.4028 || 10iter: 54.0347 sec\n",
      "iteration 5260 || Loss: 3.8101 || 10iter: 62.7126 sec\n",
      "iteration 5270 || Loss: 3.0901 || 10iter: 72.0866 sec\n",
      "iteration 5280 || Loss: 3.3405 || 10iter: 81.3324 sec\n",
      "iteration 5290 || Loss: 4.4072 || 10iter: 90.7554 sec\n",
      "iteration 5300 || Loss: 3.8581 || 10iter: 99.6750 sec\n",
      "iteration 5310 || Loss: 3.0985 || 10iter: 108.5434 sec\n",
      "iteration 5320 || Loss: 3.7002 || 10iter: 117.9669 sec\n",
      "iteration 5330 || Loss: 3.5713 || 10iter: 127.0131 sec\n",
      "iteration 5340 || Loss: 3.3930 || 10iter: 135.5975 sec\n",
      "iteration 5350 || Loss: 3.7115 || 10iter: 144.5540 sec\n",
      "iteration 5360 || Loss: 3.4613 || 10iter: 153.0452 sec\n",
      "iteration 5370 || Loss: 3.0269 || 10iter: 162.2744 sec\n",
      " (val) \n",
      "------------\n",
      "epoch 30 || Epoch_TRAIN_Loss: 648.0386 || Epoch_VAL_Loss: 724.3835\n",
      "timer: 225.1688sec\n",
      "------------\n",
      "Epoch 31/50\n",
      "------------\n",
      " (train) \n",
      "iteration 5380 || Loss: 3.7739 || 10iter: 9.0160 sec\n",
      "iteration 5390 || Loss: 3.2720 || 10iter: 18.7166 sec\n",
      "iteration 5400 || Loss: 3.5675 || 10iter: 28.2530 sec\n",
      "iteration 5410 || Loss: 3.4496 || 10iter: 36.9458 sec\n",
      "iteration 5420 || Loss: 3.7610 || 10iter: 45.9249 sec\n",
      "iteration 5430 || Loss: 3.5815 || 10iter: 54.6828 sec\n",
      "iteration 5440 || Loss: 3.3094 || 10iter: 64.3610 sec\n",
      "iteration 5450 || Loss: 3.3654 || 10iter: 73.9144 sec\n",
      "iteration 5460 || Loss: 4.0445 || 10iter: 82.4632 sec\n",
      "iteration 5470 || Loss: 3.1928 || 10iter: 91.4395 sec\n",
      "iteration 5480 || Loss: 3.8550 || 10iter: 100.1780 sec\n",
      "iteration 5490 || Loss: 3.4844 || 10iter: 109.4924 sec\n",
      "iteration 5500 || Loss: 3.4264 || 10iter: 118.9558 sec\n",
      "iteration 5510 || Loss: 3.2762 || 10iter: 128.3800 sec\n",
      "iteration 5520 || Loss: 3.3980 || 10iter: 137.7201 sec\n",
      "iteration 5530 || Loss: 3.3327 || 10iter: 146.7815 sec\n",
      "iteration 5540 || Loss: 3.1597 || 10iter: 156.5328 sec\n",
      "------------\n",
      "epoch 31 || Epoch_TRAIN_Loss: 643.6784 || Epoch_VAL_Loss: 0.0000\n",
      "timer: 164.1624sec\n",
      "------------\n",
      "Epoch 32/50\n",
      "------------\n",
      " (train) \n",
      "iteration 5550 || Loss: 3.5445 || 10iter: 0.6521 sec\n",
      "iteration 5560 || Loss: 4.1485 || 10iter: 10.2152 sec\n",
      "iteration 5570 || Loss: 3.5142 || 10iter: 19.3483 sec\n",
      "iteration 5580 || Loss: 3.4231 || 10iter: 28.3530 sec\n",
      "iteration 5590 || Loss: 3.8963 || 10iter: 36.7079 sec\n",
      "iteration 5600 || Loss: 3.8263 || 10iter: 45.3468 sec\n",
      "iteration 5610 || Loss: 3.9413 || 10iter: 54.8147 sec\n",
      "iteration 5620 || Loss: 3.1224 || 10iter: 64.0456 sec\n",
      "iteration 5630 || Loss: 3.8141 || 10iter: 73.0404 sec\n",
      "iteration 5640 || Loss: 3.5290 || 10iter: 82.2159 sec\n",
      "iteration 5650 || Loss: 3.4231 || 10iter: 90.9946 sec\n",
      "iteration 5660 || Loss: 3.2403 || 10iter: 100.0851 sec\n",
      "iteration 5670 || Loss: 3.2973 || 10iter: 109.4891 sec\n",
      "iteration 5680 || Loss: 3.4414 || 10iter: 118.5997 sec\n",
      "iteration 5690 || Loss: 3.6345 || 10iter: 127.3290 sec\n",
      "iteration 5700 || Loss: 3.4908 || 10iter: 135.6027 sec\n",
      "iteration 5710 || Loss: 3.3276 || 10iter: 144.6936 sec\n",
      "iteration 5720 || Loss: 4.4200 || 10iter: 153.8040 sec\n",
      "------------\n",
      "epoch 32 || Epoch_TRAIN_Loss: 637.6984 || Epoch_VAL_Loss: 0.0000\n",
      "timer: 161.1326sec\n",
      "------------\n",
      "Epoch 33/50\n",
      "------------\n",
      " (train) \n",
      "iteration 5730 || Loss: 3.4356 || 10iter: 1.5710 sec\n",
      "iteration 5740 || Loss: 3.4593 || 10iter: 11.2072 sec\n",
      "iteration 5750 || Loss: 3.7833 || 10iter: 20.5658 sec\n",
      "iteration 5760 || Loss: 3.6321 || 10iter: 29.1057 sec\n",
      "iteration 5770 || Loss: 3.3208 || 10iter: 37.9534 sec\n",
      "iteration 5780 || Loss: 3.3822 || 10iter: 47.6197 sec\n",
      "iteration 5790 || Loss: 2.9787 || 10iter: 56.2298 sec\n",
      "iteration 5800 || Loss: 3.1014 || 10iter: 65.1812 sec\n",
      "iteration 5810 || Loss: 4.2176 || 10iter: 73.7853 sec\n",
      "iteration 5820 || Loss: 4.1188 || 10iter: 84.2470 sec\n",
      "iteration 5830 || Loss: 3.3524 || 10iter: 93.6242 sec\n",
      "iteration 5840 || Loss: 3.5962 || 10iter: 103.0103 sec\n",
      "iteration 5850 || Loss: 3.5489 || 10iter: 111.8077 sec\n",
      "iteration 5860 || Loss: 3.5640 || 10iter: 120.8087 sec\n",
      "iteration 5870 || Loss: 3.9082 || 10iter: 129.6767 sec\n",
      "iteration 5880 || Loss: 3.7616 || 10iter: 139.6997 sec\n",
      "iteration 5890 || Loss: 3.6580 || 10iter: 149.1427 sec\n",
      "iteration 5900 || Loss: 3.5630 || 10iter: 158.3965 sec\n",
      "------------\n",
      "epoch 33 || Epoch_TRAIN_Loss: 633.4555 || Epoch_VAL_Loss: 0.0000\n",
      "timer: 164.9924sec\n",
      "------------\n",
      "Epoch 34/50\n",
      "------------\n",
      " (train) \n",
      "iteration 5910 || Loss: 3.9748 || 10iter: 2.4181 sec\n",
      "iteration 5920 || Loss: 3.2618 || 10iter: 11.4190 sec\n",
      "iteration 5930 || Loss: 3.6823 || 10iter: 20.4459 sec\n",
      "iteration 5940 || Loss: 3.4155 || 10iter: 29.1149 sec\n",
      "iteration 5950 || Loss: 3.2930 || 10iter: 38.0824 sec\n",
      "iteration 5960 || Loss: 3.3728 || 10iter: 46.7372 sec\n",
      "iteration 5970 || Loss: 3.2600 || 10iter: 55.2243 sec\n",
      "iteration 5980 || Loss: 3.3506 || 10iter: 63.4827 sec\n",
      "iteration 5990 || Loss: 3.3783 || 10iter: 72.7144 sec\n",
      "iteration 6000 || Loss: 3.7611 || 10iter: 81.7346 sec\n",
      "iteration 6010 || Loss: 4.1809 || 10iter: 90.4922 sec\n",
      "iteration 6020 || Loss: 3.7265 || 10iter: 100.3106 sec\n",
      "iteration 6030 || Loss: 2.9526 || 10iter: 109.2145 sec\n",
      "iteration 6040 || Loss: 3.3486 || 10iter: 118.6694 sec\n",
      "iteration 6050 || Loss: 3.2348 || 10iter: 127.9137 sec\n",
      "iteration 6060 || Loss: 3.1909 || 10iter: 137.0026 sec\n",
      "iteration 6070 || Loss: 3.5831 || 10iter: 145.8457 sec\n",
      "iteration 6080 || Loss: 3.4594 || 10iter: 154.7788 sec\n",
      "------------\n",
      "epoch 34 || Epoch_TRAIN_Loss: 634.0762 || Epoch_VAL_Loss: 0.0000\n",
      "timer: 160.0381sec\n",
      "------------\n",
      "Epoch 35/50\n",
      "------------\n",
      " (train) \n",
      "iteration 6090 || Loss: 3.9045 || 10iter: 3.3606 sec\n",
      "iteration 6100 || Loss: 3.7134 || 10iter: 12.4948 sec\n",
      "iteration 6110 || Loss: 3.5016 || 10iter: 21.5266 sec\n",
      "iteration 6120 || Loss: 3.6503 || 10iter: 30.2739 sec\n",
      "iteration 6130 || Loss: 3.6095 || 10iter: 39.6110 sec\n",
      "iteration 6140 || Loss: 3.8473 || 10iter: 49.4498 sec\n",
      "iteration 6150 || Loss: 3.5525 || 10iter: 58.3824 sec\n",
      "iteration 6160 || Loss: 3.0739 || 10iter: 67.3809 sec\n",
      "iteration 6170 || Loss: 3.7500 || 10iter: 75.9621 sec\n",
      "iteration 6180 || Loss: 3.5480 || 10iter: 85.9629 sec\n",
      "iteration 6190 || Loss: 3.5851 || 10iter: 95.4400 sec\n",
      "iteration 6200 || Loss: 2.6286 || 10iter: 103.8379 sec\n",
      "iteration 6210 || Loss: 4.1010 || 10iter: 112.5131 sec\n",
      "iteration 6220 || Loss: 4.0942 || 10iter: 121.1664 sec\n",
      "iteration 6230 || Loss: 3.4278 || 10iter: 130.9679 sec\n",
      "iteration 6240 || Loss: 3.4629 || 10iter: 140.2074 sec\n",
      "iteration 6250 || Loss: 3.2931 || 10iter: 149.8793 sec\n",
      "iteration 6260 || Loss: 3.3351 || 10iter: 159.5084 sec\n",
      "------------\n",
      "epoch 35 || Epoch_TRAIN_Loss: 623.9151 || Epoch_VAL_Loss: 0.0000\n",
      "timer: 163.7950sec\n",
      "------------\n",
      "Epoch 36/50\n",
      "------------\n",
      " (train) \n",
      "iteration 6270 || Loss: 3.5958 || 10iter: 4.1464 sec\n",
      "iteration 6280 || Loss: 3.2326 || 10iter: 13.2818 sec\n",
      "iteration 6290 || Loss: 3.5129 || 10iter: 22.4149 sec\n",
      "iteration 6300 || Loss: 4.1655 || 10iter: 31.2167 sec\n",
      "iteration 6310 || Loss: 3.3195 || 10iter: 40.0556 sec\n",
      "iteration 6320 || Loss: 3.7206 || 10iter: 49.3070 sec\n",
      "iteration 6330 || Loss: 3.5567 || 10iter: 58.5045 sec\n",
      "iteration 6340 || Loss: 3.2420 || 10iter: 67.6083 sec\n",
      "iteration 6350 || Loss: 3.1891 || 10iter: 76.8254 sec\n",
      "iteration 6360 || Loss: 3.4774 || 10iter: 85.8347 sec\n",
      "iteration 6370 || Loss: 3.8593 || 10iter: 94.6832 sec\n",
      "iteration 6380 || Loss: 3.1550 || 10iter: 103.6701 sec\n",
      "iteration 6390 || Loss: 3.0721 || 10iter: 112.7550 sec\n",
      "iteration 6400 || Loss: 3.3439 || 10iter: 121.6656 sec\n",
      "iteration 6410 || Loss: 3.4079 || 10iter: 130.5586 sec\n",
      "iteration 6420 || Loss: 2.6852 || 10iter: 138.9073 sec\n",
      "iteration 6430 || Loss: 4.0031 || 10iter: 147.7457 sec\n",
      "iteration 6440 || Loss: 3.3737 || 10iter: 156.8674 sec\n",
      "------------\n",
      "epoch 36 || Epoch_TRAIN_Loss: 624.3020 || Epoch_VAL_Loss: 0.0000\n",
      "timer: 160.3088sec\n",
      "------------\n",
      "Epoch 37/50\n",
      "------------\n",
      " (train) \n",
      "iteration 6450 || Loss: 3.9704 || 10iter: 5.3162 sec\n",
      "iteration 6460 || Loss: 3.6305 || 10iter: 13.7708 sec\n",
      "iteration 6470 || Loss: 3.4108 || 10iter: 22.9091 sec\n",
      "iteration 6480 || Loss: 3.1302 || 10iter: 31.3662 sec\n",
      "iteration 6490 || Loss: 4.0068 || 10iter: 40.8234 sec\n",
      "iteration 6500 || Loss: 3.5688 || 10iter: 49.3724 sec\n",
      "iteration 6510 || Loss: 3.5732 || 10iter: 58.6442 sec\n",
      "iteration 6520 || Loss: 3.3196 || 10iter: 67.8431 sec\n",
      "iteration 6530 || Loss: 3.5545 || 10iter: 77.0085 sec\n",
      "iteration 6540 || Loss: 3.8522 || 10iter: 86.3495 sec\n",
      "iteration 6550 || Loss: 3.3849 || 10iter: 95.3107 sec\n",
      "iteration 6560 || Loss: 3.4756 || 10iter: 104.2237 sec\n",
      "iteration 6570 || Loss: 3.5850 || 10iter: 113.8961 sec\n",
      "iteration 6580 || Loss: 3.7796 || 10iter: 123.2817 sec\n",
      "iteration 6590 || Loss: 3.0327 || 10iter: 133.1093 sec\n",
      "iteration 6600 || Loss: 3.0117 || 10iter: 142.0151 sec\n",
      "iteration 6610 || Loss: 3.3683 || 10iter: 150.9200 sec\n",
      "iteration 6620 || Loss: 3.3492 || 10iter: 160.1695 sec\n",
      "------------\n",
      "epoch 37 || Epoch_TRAIN_Loss: 621.0618 || Epoch_VAL_Loss: 0.0000\n",
      "timer: 162.7632sec\n",
      "------------\n",
      "Epoch 38/50\n",
      "------------\n",
      " (train) \n",
      "iteration 6630 || Loss: 3.4354 || 10iter: 5.7641 sec\n",
      "iteration 6640 || Loss: 3.8218 || 10iter: 15.1332 sec\n",
      "iteration 6650 || Loss: 3.6451 || 10iter: 24.8385 sec\n",
      "iteration 6660 || Loss: 3.9020 || 10iter: 34.3278 sec\n",
      "iteration 6670 || Loss: 3.5734 || 10iter: 43.5047 sec\n",
      "iteration 6680 || Loss: 3.3546 || 10iter: 52.2382 sec\n",
      "iteration 6690 || Loss: 3.3178 || 10iter: 61.0783 sec\n",
      "iteration 6700 || Loss: 3.7922 || 10iter: 70.3492 sec\n",
      "iteration 6710 || Loss: 3.3806 || 10iter: 79.3271 sec\n",
      "iteration 6720 || Loss: 3.3810 || 10iter: 88.2408 sec\n",
      "iteration 6730 || Loss: 3.7399 || 10iter: 97.8694 sec\n",
      "iteration 6740 || Loss: 3.9031 || 10iter: 107.1224 sec\n",
      "iteration 6750 || Loss: 3.5670 || 10iter: 116.3231 sec\n",
      "iteration 6760 || Loss: 3.4301 || 10iter: 124.9129 sec\n",
      "iteration 6770 || Loss: 3.8397 || 10iter: 134.0120 sec\n",
      "iteration 6780 || Loss: 3.9538 || 10iter: 142.7404 sec\n",
      "iteration 6790 || Loss: 3.3284 || 10iter: 151.2900 sec\n",
      "iteration 6800 || Loss: 3.0868 || 10iter: 160.2452 sec\n",
      "------------\n",
      "epoch 38 || Epoch_TRAIN_Loss: 620.2989 || Epoch_VAL_Loss: 0.0000\n",
      "timer: 161.9081sec\n",
      "------------\n",
      "Epoch 39/50\n",
      "------------\n",
      " (train) \n",
      "iteration 6810 || Loss: 3.5864 || 10iter: 6.5837 sec\n",
      "iteration 6820 || Loss: 3.6120 || 10iter: 16.2039 sec\n",
      "iteration 6830 || Loss: 3.0806 || 10iter: 25.0164 sec\n",
      "iteration 6840 || Loss: 3.1209 || 10iter: 33.6402 sec\n",
      "iteration 6850 || Loss: 3.0281 || 10iter: 43.0031 sec\n",
      "iteration 6860 || Loss: 3.5951 || 10iter: 52.5654 sec\n",
      "iteration 6870 || Loss: 3.2177 || 10iter: 61.8206 sec\n",
      "iteration 6880 || Loss: 3.9286 || 10iter: 70.6200 sec\n",
      "iteration 6890 || Loss: 3.0627 || 10iter: 79.4515 sec\n",
      "iteration 6900 || Loss: 3.6765 || 10iter: 89.0910 sec\n",
      "iteration 6910 || Loss: 3.7460 || 10iter: 98.7086 sec\n",
      "iteration 6920 || Loss: 3.5559 || 10iter: 107.7821 sec\n",
      "iteration 6930 || Loss: 3.6305 || 10iter: 117.0764 sec\n",
      "iteration 6940 || Loss: 3.0544 || 10iter: 126.8531 sec\n",
      "iteration 6950 || Loss: 3.6078 || 10iter: 136.4016 sec\n",
      "iteration 6960 || Loss: 3.4988 || 10iter: 145.4141 sec\n",
      "iteration 6970 || Loss: 3.3601 || 10iter: 154.1659 sec\n",
      "iteration 6980 || Loss: 3.5919 || 10iter: 163.1713 sec\n",
      "------------\n",
      "epoch 39 || Epoch_TRAIN_Loss: 616.7360 || Epoch_VAL_Loss: 0.0000\n",
      "timer: 163.9587sec\n",
      "------------\n",
      "Epoch 40/50\n",
      "------------\n",
      " (train) \n",
      "iteration 6990 || Loss: 3.0601 || 10iter: 7.4575 sec\n",
      "iteration 7000 || Loss: 2.9260 || 10iter: 16.3340 sec\n",
      "iteration 7010 || Loss: 3.1072 || 10iter: 25.9355 sec\n",
      "iteration 7020 || Loss: 3.4595 || 10iter: 36.2626 sec\n",
      "iteration 7030 || Loss: 3.1004 || 10iter: 44.7214 sec\n",
      "iteration 7040 || Loss: 4.0204 || 10iter: 53.9233 sec\n",
      "iteration 7050 || Loss: 2.7766 || 10iter: 62.7842 sec\n",
      "iteration 7060 || Loss: 3.5914 || 10iter: 73.2275 sec\n",
      "iteration 7070 || Loss: 3.5607 || 10iter: 82.0516 sec\n",
      "iteration 7080 || Loss: 3.4099 || 10iter: 91.7196 sec\n",
      "iteration 7090 || Loss: 3.4710 || 10iter: 100.7795 sec\n",
      "iteration 7100 || Loss: 3.5368 || 10iter: 109.6520 sec\n",
      "iteration 7110 || Loss: 3.4038 || 10iter: 118.4423 sec\n",
      "iteration 7120 || Loss: 3.4312 || 10iter: 127.8428 sec\n",
      "iteration 7130 || Loss: 3.5681 || 10iter: 136.7288 sec\n",
      "iteration 7140 || Loss: 3.3701 || 10iter: 145.7356 sec\n",
      "iteration 7150 || Loss: 3.3873 || 10iter: 154.5969 sec\n",
      "iteration 7160 || Loss: 2.9439 || 10iter: 163.8778 sec\n",
      " (val) \n",
      "------------\n",
      "epoch 40 || Epoch_TRAIN_Loss: 607.5940 || Epoch_VAL_Loss: 686.5854\n",
      "timer: 229.5004sec\n",
      "------------\n",
      "Epoch 41/50\n",
      "------------\n",
      " (train) \n",
      "iteration 7170 || Loss: 3.6817 || 10iter: 8.4111 sec\n",
      "iteration 7180 || Loss: 3.0663 || 10iter: 17.1972 sec\n",
      "iteration 7190 || Loss: 3.5824 || 10iter: 26.5968 sec\n",
      "iteration 7200 || Loss: 3.1694 || 10iter: 36.2264 sec\n",
      "iteration 7210 || Loss: 3.4715 || 10iter: 45.4298 sec\n",
      "iteration 7220 || Loss: 3.1915 || 10iter: 54.5141 sec\n",
      "iteration 7230 || Loss: 3.6072 || 10iter: 63.2657 sec\n",
      "iteration 7240 || Loss: 3.3339 || 10iter: 72.5410 sec\n",
      "iteration 7250 || Loss: 3.3959 || 10iter: 81.4528 sec\n",
      "iteration 7260 || Loss: 2.9853 || 10iter: 90.5328 sec\n",
      "iteration 7270 || Loss: 3.6743 || 10iter: 100.3717 sec\n",
      "iteration 7280 || Loss: 3.4505 || 10iter: 109.3265 sec\n",
      "iteration 7290 || Loss: 3.2924 || 10iter: 118.0106 sec\n",
      "iteration 7300 || Loss: 3.2780 || 10iter: 126.7101 sec\n",
      "iteration 7310 || Loss: 3.5785 || 10iter: 135.3689 sec\n",
      "iteration 7320 || Loss: 3.0723 || 10iter: 144.4736 sec\n",
      "iteration 7330 || Loss: 3.5026 || 10iter: 153.4061 sec\n",
      "------------\n",
      "epoch 41 || Epoch_TRAIN_Loss: 603.0224 || Epoch_VAL_Loss: 0.0000\n",
      "timer: 161.3453sec\n",
      "------------\n",
      "Epoch 42/50\n",
      "------------\n",
      " (train) \n",
      "iteration 7340 || Loss: 3.4541 || 10iter: 0.6347 sec\n",
      "iteration 7350 || Loss: 3.1266 || 10iter: 9.4240 sec\n",
      "iteration 7360 || Loss: 3.4598 || 10iter: 18.1245 sec\n",
      "iteration 7370 || Loss: 3.4232 || 10iter: 27.2021 sec\n",
      "iteration 7380 || Loss: 3.4401 || 10iter: 36.6511 sec\n",
      "iteration 7390 || Loss: 2.6062 || 10iter: 45.5409 sec\n",
      "iteration 7400 || Loss: 3.3818 || 10iter: 55.0960 sec\n",
      "iteration 7410 || Loss: 3.1901 || 10iter: 64.3911 sec\n",
      "iteration 7420 || Loss: 3.6050 || 10iter: 73.5773 sec\n",
      "iteration 7430 || Loss: 3.3884 || 10iter: 83.7260 sec\n",
      "iteration 7440 || Loss: 3.6832 || 10iter: 93.6462 sec\n",
      "iteration 7450 || Loss: 4.0088 || 10iter: 101.9759 sec\n",
      "iteration 7460 || Loss: 3.0929 || 10iter: 111.4882 sec\n",
      "iteration 7470 || Loss: 3.0219 || 10iter: 120.3710 sec\n",
      "iteration 7480 || Loss: 3.8192 || 10iter: 129.9056 sec\n",
      "iteration 7490 || Loss: 3.2099 || 10iter: 138.8023 sec\n",
      "iteration 7500 || Loss: 3.3349 || 10iter: 148.3202 sec\n",
      "iteration 7510 || Loss: 2.9869 || 10iter: 157.0417 sec\n",
      "------------\n",
      "epoch 42 || Epoch_TRAIN_Loss: 606.3526 || Epoch_VAL_Loss: 0.0000\n",
      "timer: 164.5582sec\n",
      "------------\n",
      "Epoch 43/50\n",
      "------------\n",
      " (train) \n",
      "iteration 7520 || Loss: 3.4008 || 10iter: 1.3966 sec\n",
      "iteration 7530 || Loss: 3.2177 || 10iter: 10.0018 sec\n",
      "iteration 7540 || Loss: 3.6995 || 10iter: 18.4712 sec\n",
      "iteration 7550 || Loss: 2.9925 || 10iter: 27.2713 sec\n",
      "iteration 7560 || Loss: 3.2816 || 10iter: 36.3025 sec\n",
      "iteration 7570 || Loss: 2.4651 || 10iter: 45.5245 sec\n",
      "iteration 7580 || Loss: 3.5494 || 10iter: 54.5795 sec\n",
      "iteration 7590 || Loss: 3.1829 || 10iter: 63.6688 sec\n",
      "iteration 7600 || Loss: 2.9584 || 10iter: 72.7040 sec\n",
      "iteration 7610 || Loss: 2.7361 || 10iter: 81.5832 sec\n",
      "iteration 7620 || Loss: 3.7372 || 10iter: 90.7069 sec\n",
      "iteration 7630 || Loss: 3.4087 || 10iter: 100.3423 sec\n",
      "iteration 7640 || Loss: 3.4935 || 10iter: 109.9301 sec\n",
      "iteration 7650 || Loss: 3.6644 || 10iter: 119.2456 sec\n",
      "iteration 7660 || Loss: 3.4309 || 10iter: 127.8747 sec\n",
      "iteration 7670 || Loss: 3.8171 || 10iter: 136.7004 sec\n",
      "iteration 7680 || Loss: 3.0393 || 10iter: 146.2182 sec\n",
      "iteration 7690 || Loss: 3.0487 || 10iter: 155.1536 sec\n",
      "------------\n",
      "epoch 43 || Epoch_TRAIN_Loss: 597.7459 || Epoch_VAL_Loss: 0.0000\n",
      "timer: 161.5149sec\n",
      "------------\n",
      "Epoch 44/50\n",
      "------------\n",
      " (train) \n",
      "iteration 7700 || Loss: 3.4370 || 10iter: 2.5424 sec\n",
      "iteration 7710 || Loss: 3.3209 || 10iter: 12.0715 sec\n",
      "iteration 7720 || Loss: 3.4718 || 10iter: 21.0757 sec\n",
      "iteration 7730 || Loss: 2.8794 || 10iter: 29.8348 sec\n",
      "iteration 7740 || Loss: 3.1267 || 10iter: 39.0240 sec\n",
      "iteration 7750 || Loss: 3.0829 || 10iter: 48.2825 sec\n",
      "iteration 7760 || Loss: 3.6108 || 10iter: 57.5609 sec\n",
      "iteration 7770 || Loss: 3.2628 || 10iter: 66.8418 sec\n",
      "iteration 7780 || Loss: 2.8878 || 10iter: 76.3490 sec\n",
      "iteration 7790 || Loss: 3.4748 || 10iter: 85.4712 sec\n",
      "iteration 7800 || Loss: 2.5558 || 10iter: 94.1952 sec\n",
      "iteration 7810 || Loss: 2.8853 || 10iter: 103.8087 sec\n",
      "iteration 7820 || Loss: 3.7114 || 10iter: 113.5850 sec\n",
      "iteration 7830 || Loss: 3.6465 || 10iter: 123.2398 sec\n",
      "iteration 7840 || Loss: 3.5270 || 10iter: 132.6886 sec\n",
      "iteration 7850 || Loss: 3.7184 || 10iter: 141.8830 sec\n",
      "iteration 7860 || Loss: 3.2805 || 10iter: 151.1759 sec\n",
      "iteration 7870 || Loss: 3.5428 || 10iter: 160.2649 sec\n",
      "------------\n",
      "epoch 44 || Epoch_TRAIN_Loss: 597.8377 || Epoch_VAL_Loss: 0.0000\n",
      "timer: 165.4314sec\n",
      "------------\n",
      "Epoch 45/50\n",
      "------------\n",
      " (train) \n",
      "iteration 7880 || Loss: 3.1863 || 10iter: 3.4798 sec\n",
      "iteration 7890 || Loss: 2.7171 || 10iter: 12.9126 sec\n",
      "iteration 7900 || Loss: 3.5535 || 10iter: 22.0548 sec\n",
      "iteration 7910 || Loss: 3.1124 || 10iter: 31.5142 sec\n",
      "iteration 7920 || Loss: 4.0624 || 10iter: 40.3274 sec\n",
      "iteration 7930 || Loss: 3.4576 || 10iter: 49.2052 sec\n",
      "iteration 7940 || Loss: 3.5598 || 10iter: 58.0089 sec\n",
      "iteration 7950 || Loss: 3.5529 || 10iter: 66.9351 sec\n",
      "iteration 7960 || Loss: 3.1487 || 10iter: 75.9596 sec\n",
      "iteration 7970 || Loss: 3.0759 || 10iter: 85.9275 sec\n",
      "iteration 7980 || Loss: 3.3939 || 10iter: 94.5229 sec\n",
      "iteration 7990 || Loss: 3.1449 || 10iter: 103.7435 sec\n",
      "iteration 8000 || Loss: 3.0525 || 10iter: 112.7732 sec\n",
      "iteration 8010 || Loss: 3.3657 || 10iter: 121.9143 sec\n",
      "iteration 8020 || Loss: 3.5511 || 10iter: 131.3460 sec\n",
      "iteration 8030 || Loss: 3.5485 || 10iter: 140.6598 sec\n",
      "iteration 8040 || Loss: 3.1406 || 10iter: 149.3668 sec\n",
      "iteration 8050 || Loss: 3.3826 || 10iter: 158.2642 sec\n",
      "------------\n",
      "epoch 45 || Epoch_TRAIN_Loss: 600.3168 || Epoch_VAL_Loss: 0.0000\n",
      "timer: 162.6011sec\n",
      "------------\n",
      "Epoch 46/50\n",
      "------------\n",
      " (train) \n",
      "iteration 8060 || Loss: 3.3169 || 10iter: 4.1354 sec\n",
      "iteration 8070 || Loss: 3.2886 || 10iter: 13.5173 sec\n",
      "iteration 8080 || Loss: 3.1601 || 10iter: 22.8623 sec\n",
      "iteration 8090 || Loss: 3.2230 || 10iter: 32.7027 sec\n",
      "iteration 8100 || Loss: 3.8381 || 10iter: 41.3492 sec\n",
      "iteration 8110 || Loss: 3.2485 || 10iter: 50.5023 sec\n",
      "iteration 8120 || Loss: 2.8409 || 10iter: 59.6012 sec\n",
      "iteration 8130 || Loss: 3.3075 || 10iter: 69.3078 sec\n",
      "iteration 8140 || Loss: 3.3139 || 10iter: 78.5914 sec\n",
      "iteration 8150 || Loss: 3.3812 || 10iter: 88.2104 sec\n",
      "iteration 8160 || Loss: 3.2049 || 10iter: 97.6077 sec\n",
      "iteration 8170 || Loss: 3.0060 || 10iter: 106.7373 sec\n",
      "iteration 8180 || Loss: 3.7256 || 10iter: 116.6637 sec\n",
      "iteration 8190 || Loss: 3.5273 || 10iter: 125.6211 sec\n",
      "iteration 8200 || Loss: 3.1654 || 10iter: 134.7050 sec\n",
      "iteration 8210 || Loss: 2.9348 || 10iter: 144.1154 sec\n",
      "iteration 8220 || Loss: 3.5590 || 10iter: 153.7421 sec\n",
      "iteration 8230 || Loss: 3.4494 || 10iter: 162.6106 sec\n",
      "------------\n",
      "epoch 46 || Epoch_TRAIN_Loss: 591.7362 || Epoch_VAL_Loss: 0.0000\n",
      "timer: 165.8441sec\n",
      "------------\n",
      "Epoch 47/50\n",
      "------------\n",
      " (train) \n",
      "iteration 8240 || Loss: 3.5157 || 10iter: 5.2804 sec\n",
      "iteration 8250 || Loss: 3.3785 || 10iter: 14.0438 sec\n",
      "iteration 8260 || Loss: 2.9373 || 10iter: 22.9606 sec\n",
      "iteration 8270 || Loss: 3.0381 || 10iter: 32.0925 sec\n",
      "iteration 8280 || Loss: 3.0919 || 10iter: 41.1898 sec\n",
      "iteration 8290 || Loss: 2.9321 || 10iter: 49.9349 sec\n",
      "iteration 8300 || Loss: 3.0397 || 10iter: 58.0250 sec\n",
      "iteration 8310 || Loss: 2.7168 || 10iter: 67.1203 sec\n",
      "iteration 8320 || Loss: 3.3004 || 10iter: 75.8679 sec\n",
      "iteration 8330 || Loss: 3.1693 || 10iter: 86.1464 sec\n",
      "iteration 8340 || Loss: 3.4733 || 10iter: 95.5669 sec\n",
      "iteration 8350 || Loss: 2.8507 || 10iter: 105.9898 sec\n",
      "iteration 8360 || Loss: 3.6571 || 10iter: 115.3268 sec\n",
      "iteration 8370 || Loss: 3.5860 || 10iter: 124.0574 sec\n",
      "iteration 8380 || Loss: 3.4607 || 10iter: 132.7592 sec\n",
      "iteration 8390 || Loss: 3.1862 || 10iter: 141.8315 sec\n",
      "iteration 8400 || Loss: 3.1606 || 10iter: 150.9647 sec\n",
      "iteration 8410 || Loss: 3.8767 || 10iter: 159.9468 sec\n",
      "------------\n",
      "epoch 47 || Epoch_TRAIN_Loss: 587.9839 || Epoch_VAL_Loss: 0.0000\n",
      "timer: 162.3889sec\n",
      "------------\n",
      "Epoch 48/50\n",
      "------------\n",
      " (train) \n",
      "iteration 8420 || Loss: 3.0085 || 10iter: 5.6142 sec\n",
      "iteration 8430 || Loss: 2.9060 || 10iter: 14.1925 sec\n",
      "iteration 8440 || Loss: 3.8428 || 10iter: 23.5760 sec\n",
      "iteration 8450 || Loss: 3.7189 || 10iter: 32.7276 sec\n",
      "iteration 8460 || Loss: 3.6077 || 10iter: 41.7150 sec\n",
      "iteration 8470 || Loss: 3.1398 || 10iter: 51.6290 sec\n",
      "iteration 8480 || Loss: 3.4309 || 10iter: 60.4174 sec\n",
      "iteration 8490 || Loss: 3.1763 || 10iter: 69.4564 sec\n",
      "iteration 8500 || Loss: 3.4701 || 10iter: 78.1822 sec\n",
      "iteration 8510 || Loss: 3.4972 || 10iter: 87.2075 sec\n",
      "iteration 8520 || Loss: 2.8867 || 10iter: 96.0571 sec\n",
      "iteration 8530 || Loss: 3.3966 || 10iter: 105.1023 sec\n",
      "iteration 8540 || Loss: 3.6212 || 10iter: 114.3863 sec\n",
      "iteration 8550 || Loss: 3.5519 || 10iter: 124.0360 sec\n",
      "iteration 8560 || Loss: 2.8198 || 10iter: 132.7435 sec\n",
      "iteration 8570 || Loss: 3.0649 || 10iter: 142.0810 sec\n",
      "iteration 8580 || Loss: 3.2998 || 10iter: 150.7578 sec\n",
      "iteration 8590 || Loss: 3.4875 || 10iter: 160.4137 sec\n",
      "------------\n",
      "epoch 48 || Epoch_TRAIN_Loss: 588.9560 || Epoch_VAL_Loss: 0.0000\n",
      "timer: 162.2773sec\n",
      "------------\n",
      "Epoch 49/50\n",
      "------------\n",
      " (train) \n",
      "iteration 8600 || Loss: 3.1281 || 10iter: 7.1442 sec\n",
      "iteration 8610 || Loss: 2.9227 || 10iter: 15.8005 sec\n",
      "iteration 8620 || Loss: 3.3369 || 10iter: 24.7935 sec\n",
      "iteration 8630 || Loss: 3.5301 || 10iter: 34.1617 sec\n",
      "iteration 8640 || Loss: 2.8586 || 10iter: 43.0733 sec\n",
      "iteration 8650 || Loss: 3.1459 || 10iter: 52.0161 sec\n",
      "iteration 8660 || Loss: 2.9696 || 10iter: 61.1085 sec\n",
      "iteration 8670 || Loss: 3.6247 || 10iter: 70.7078 sec\n",
      "iteration 8680 || Loss: 3.3479 || 10iter: 79.2310 sec\n",
      "iteration 8690 || Loss: 3.4544 || 10iter: 87.6903 sec\n",
      "iteration 8700 || Loss: 3.3944 || 10iter: 96.0129 sec\n",
      "iteration 8710 || Loss: 3.4286 || 10iter: 105.7235 sec\n",
      "iteration 8720 || Loss: 3.1173 || 10iter: 115.6111 sec\n",
      "iteration 8730 || Loss: 3.5075 || 10iter: 124.8407 sec\n",
      "iteration 8740 || Loss: 3.3868 || 10iter: 134.8364 sec\n",
      "iteration 8750 || Loss: 3.2154 || 10iter: 143.6095 sec\n",
      "iteration 8760 || Loss: 3.9395 || 10iter: 152.4947 sec\n",
      "iteration 8770 || Loss: 3.2269 || 10iter: 162.5087 sec\n",
      "------------\n",
      "epoch 49 || Epoch_TRAIN_Loss: 579.7453 || Epoch_VAL_Loss: 0.0000\n",
      "timer: 163.3638sec\n",
      "------------\n",
      "Epoch 50/50\n",
      "------------\n",
      " (train) \n",
      "iteration 8780 || Loss: 2.8793 || 10iter: 8.4808 sec\n",
      "iteration 8790 || Loss: 2.9669 || 10iter: 17.5685 sec\n",
      "iteration 8800 || Loss: 3.1701 || 10iter: 26.5594 sec\n",
      "iteration 8810 || Loss: 3.1423 || 10iter: 35.7892 sec\n",
      "iteration 8820 || Loss: 3.3819 || 10iter: 45.1469 sec\n",
      "iteration 8830 || Loss: 2.9967 || 10iter: 53.9936 sec\n",
      "iteration 8840 || Loss: 3.3237 || 10iter: 64.1590 sec\n",
      "iteration 8850 || Loss: 2.9590 || 10iter: 72.5038 sec\n",
      "iteration 8860 || Loss: 3.7395 || 10iter: 81.2037 sec\n",
      "iteration 8870 || Loss: 3.3583 || 10iter: 90.2148 sec\n",
      "iteration 8880 || Loss: 3.8132 || 10iter: 99.5773 sec\n",
      "iteration 8890 || Loss: 4.0016 || 10iter: 108.4322 sec\n",
      "iteration 8900 || Loss: 3.4909 || 10iter: 117.8955 sec\n",
      "iteration 8910 || Loss: 3.5954 || 10iter: 126.7066 sec\n",
      "iteration 8920 || Loss: 3.3397 || 10iter: 136.0464 sec\n",
      "iteration 8930 || Loss: 3.1271 || 10iter: 146.2090 sec\n",
      "iteration 8940 || Loss: 3.3415 || 10iter: 154.9846 sec\n",
      "iteration 8950 || Loss: 2.9591 || 10iter: 164.9913 sec\n",
      " (val) \n",
      "------------\n",
      "epoch 50 || Epoch_TRAIN_Loss: 576.8685 || Epoch_VAL_Loss: 673.0902\n",
      "timer: 228.0282sec\n",
      "------------\n",
      "Epoch 51/50\n",
      "------------\n",
      " (train) \n",
      "iteration 8960 || Loss: 3.1616 || 10iter: 8.6106 sec\n",
      "iteration 8970 || Loss: 3.1613 || 10iter: 17.5653 sec\n",
      "iteration 8980 || Loss: 2.6113 || 10iter: 26.4248 sec\n",
      "iteration 8990 || Loss: 3.3201 || 10iter: 34.8882 sec\n",
      "iteration 9000 || Loss: 3.1337 || 10iter: 44.1584 sec\n",
      "iteration 9010 || Loss: 3.4342 || 10iter: 53.3561 sec\n",
      "iteration 9020 || Loss: 2.8656 || 10iter: 62.7043 sec\n",
      "iteration 9030 || Loss: 3.2617 || 10iter: 71.7637 sec\n",
      "iteration 9040 || Loss: 3.5219 || 10iter: 80.4098 sec\n",
      "iteration 9050 || Loss: 3.0407 || 10iter: 89.7174 sec\n",
      "iteration 9060 || Loss: 3.4254 || 10iter: 98.6898 sec\n",
      "iteration 9070 || Loss: 3.3072 || 10iter: 107.7283 sec\n",
      "iteration 9080 || Loss: 3.5129 || 10iter: 116.7232 sec\n",
      "iteration 9090 || Loss: 3.1888 || 10iter: 125.7809 sec\n",
      "iteration 9100 || Loss: 2.9967 || 10iter: 136.0342 sec\n",
      "iteration 9110 || Loss: 3.0741 || 10iter: 144.6791 sec\n",
      "iteration 9120 || Loss: 3.3007 || 10iter: 153.8759 sec\n",
      "------------\n",
      "epoch 51 || Epoch_TRAIN_Loss: 578.1089 || Epoch_VAL_Loss: 0.0000\n",
      "timer: 161.6223sec\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 50\n",
    "train_model(net, dataloaders_dict, criterion, optimizer, num_epochs=num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:new]",
   "language": "python",
   "name": "conda-env-new-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
