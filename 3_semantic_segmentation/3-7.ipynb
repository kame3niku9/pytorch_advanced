{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import math\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as data\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 1234\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FineTuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.dataloader import make_datapath_list, DataTransform, VOCDataset\n",
    "\n",
    "# file path list の作成\n",
    "rootpath = './data/VOCdevkit/VOC2012/'\n",
    "train_img_list, train_anno_list, val_img_list, val_anno_list = make_datapath_list(rootpath=rootpath)\n",
    "\n",
    "# Dataset作成\n",
    "color_mean = (0.485, 0.456, 0.406)\n",
    "color_std = (0.229, 0.224, 0.225)\n",
    "\n",
    "train_dataset = VOCDataset(train_img_list, train_anno_list, phase='train',\n",
    "                          transform=DataTransform(input_size=475, color_mean=color_mean, color_std=color_std))\n",
    "\n",
    "val_dataset = VOCDataset(val_img_list, val_anno_list, phase='val', \n",
    "                        transform=DataTransform(input_size=475, color_mean=color_mean, color_std=color_std))\n",
    "\n",
    "batch_size = 8\n",
    "\n",
    "train_dataloader = data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_dataloader = data.DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "dataloaders_dict = {'train': train_dataloader, 'val': val_dataloader}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Conv2d(256, 21, kernel_size=(1, 1), stride=(1, 1))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from utils.pspnet import PSPNet\n",
    "\n",
    "# ADE20Kのpretrainを読み込み\n",
    "net = PSPNet(n_classes=150)\n",
    "\n",
    "state_dict = torch.load('./weights/pspnet50_ADE20K.pth')\n",
    "net.load_state_dict(state_dict)\n",
    "\n",
    "# 分類用の畳み込み層を21chのものに付け替え\n",
    "n_classes = 21\n",
    "net.decode_feature.classification = nn.Conv2d(in_channels=512, out_channels=n_classes, kernel_size=1, stride=1, padding=0)\n",
    "net.aux.classification = nn.Conv2d(in_channels=256, out_channels=n_classes, kernel_size=1, stride=1, padding=0)\n",
    "\n",
    "# 付け替えた畳み込み層を初期化する。活性化関数がシグモイド関数なのでXavierを使用する (Heじゃないよ)\n",
    "def weights_init(m):\n",
    "    if isinstance(m, nn.Conv2d):\n",
    "        nn.init.xavier_normal_(m.weight.data)\n",
    "        if m.bias is not None:\n",
    "            nn.init.constant_(m.bias, 0.0)\n",
    "            \n",
    "net.decode_feature.classification.apply(weights_init)\n",
    "net.aux.classification.apply(weights_init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 損失関数の設定\n",
    "class PSPLoss(nn.Module):\n",
    "    def __init__(self, aux_weight=0.4):\n",
    "        super(PSPLoss, self).__init__()\n",
    "        self.aux_weight = aux_weight\n",
    "        \n",
    "    def forward(self, outputs, targets):\n",
    "        '''\n",
    "        Parameters\n",
    "        ----------\n",
    "        outputs: PSPNetの出力 (tuple)\n",
    "            (output=torch.Size([num_batch, 21, 475, 475]), \n",
    "             output_aux=torch.Size([num_batch, 21, 475, 475]))\n",
    "             \n",
    "        targets: [num_batch, 475, 475]\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        loss: テンソル 損失の値\n",
    "        '''\n",
    "        loss = F.cross_entropy(outputs[0], targets, reduction='mean')\n",
    "        loss_aux = F.cross_entropy(outputs[1], targets, reduction='mean')\n",
    "        \n",
    "        return loss+self.aux_weight*loss_aux\n",
    "\n",
    "criterion = PSPLoss(aux_weight=0.4)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "スケジューラを利用したepochごとの学習率の変更"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.SGD([\n",
    "    {'params': net.feature_conv.parameters(), 'lr': 1e-3},\n",
    "    {'params': net.feature_res_1.parameters(), 'lr': 1e-3},\n",
    "    {'params': net.feature_res_2.parameters(), 'lr': 1e-3},\n",
    "    {'params': net.feature_dilated_res_1.parameters(), 'lr': 1e-3},\n",
    "    {'params': net.feature_dilated_res_2.parameters(), 'lr': 1e-3},\n",
    "    {'params': net.pyramid_pooling.parameters(), 'lr': 1e-3},\n",
    "    {'params': net.decode_feature.parameters(), 'lr': 1e-2},\n",
    "    {'params': net.aux.parameters(), 'lr': 1e-2},\n",
    "], momentum=0.9, weight_decay=0.0001)\n",
    "\n",
    "def lambda_epoch(epoch):\n",
    "    max_epoch = 30\n",
    "    return math.pow((1-epoch/max_epoch), 0.9)\n",
    "    \n",
    "scheduler = optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lambda_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# モデルを学習させる関数\n",
    "def train_model(net, dataloaders_dict, criterion, scheduler, optimizer, num_epochs):\n",
    "    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "    net.to(device)\n",
    "    \n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    \n",
    "    num_train_imgs = len(dataloaders_dict['train'].dataset)\n",
    "    num_val_imgs = len(dataloaders_dict['val'].dataset)\n",
    "    batch_size = dataloaders_dict['train'].batch_size\n",
    "    \n",
    "    iteration = 1\n",
    "    logs = []\n",
    "    \n",
    "    batch_multiplier = 3\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        t_epoch_start = time.time()\n",
    "        t_iter_start = time.time()\n",
    "        epoch_train_loss = 0.0\n",
    "        epoch_val_loss = 0.0\n",
    "        \n",
    "        print(f'Epoch {epoch+1}/{num_epochs}')\n",
    "        \n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                net.train()\n",
    "                scheduler.step()\n",
    "                optimizer.zero_grad()\n",
    "                print(' (train) ')\n",
    "                \n",
    "            else:\n",
    "                if ((epoch+1) % 5 == 0):\n",
    "                    net.eval()\n",
    "                    print('---------')\n",
    "                    print(' (val) ')\n",
    "                else:\n",
    "                    continue\n",
    "                    \n",
    "            count = 0\n",
    "            for imgs, anno_class_imgs in dataloaders_dict[phase]:\n",
    "                if imgs.size()[0] == 1:\n",
    "                    continue\n",
    "                    \n",
    "                imgs = imgs.to(device)\n",
    "                anno_class_imgs = anno_class_imgs.to(device)\n",
    "                \n",
    "                # multiple minibatchでのパラメタ更新\n",
    "                if (phase == 'train') and (count == 0):\n",
    "                    optimizer.step()\n",
    "                    optimizer.zero_grad()\n",
    "                    count = batch_multiplier\n",
    "                    \n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = net(imgs)\n",
    "                    loss = criterion(outputs, anno_class_imgs.long()) / batch_multiplier\n",
    "                    \n",
    "                    # 訓練時はbackpropagation\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        count -=1\n",
    "                        \n",
    "                        if (iteration % 10 == 0):\n",
    "                            t_iter_finish = time.time()\n",
    "                            duration = t_iter_finish - t_iter_start\n",
    "                            print(f'iteration {iteration} || \\\n",
    "                            Loss: {loss.item()/batch_size*batch_multiplier:.4f} || \\\n",
    "                            10iter: {duration:.4f}sec.')\n",
    "                        \n",
    "                        epoch_train_loss += loss.item() * batch_multiplier\n",
    "                        iteration += 1\n",
    "                    \n",
    "                    else:\n",
    "                        epoch_val_loss += loss.item() * batch_multiplier\n",
    "                        \n",
    "        t_epoch_finish = time.time()\n",
    "        print('------------')\n",
    "        print(f'epoch {epoch+1} || epoch_train_loss: {epoch_train_loss/num_train_imgs:.4f} \\\n",
    "        || epoch_val_loss: {epoch_val_loss/num_val_imgs:.4f}')\n",
    "        t_epoch_start = time.time()\n",
    "        \n",
    "        log_epoch = {'epoch': epoch+1, 'train_loss': epoch_train_loss/num_train_imgs,\n",
    "                    'val_loss': epoch_val_loss/num_val_imgs}\n",
    "        logs.append(log_epoch)\n",
    "        df = pd.DataFrame(logs)\n",
    "        df.to_csv('log_output.csv')\n",
    "        \n",
    "    torch.save(net.state_dict(), 'weights/pspnet50_' + str(epoch+1) + '.pth')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      " (train) \n",
      "iteration 10 ||                             Loss: 0.3835 ||                             10iter: 14.5618sec.\n",
      "iteration 20 ||                             Loss: 0.2189 ||                             10iter: 23.2726sec.\n",
      "iteration 30 ||                             Loss: 0.1510 ||                             10iter: 31.8795sec.\n",
      "iteration 40 ||                             Loss: 0.1658 ||                             10iter: 40.5575sec.\n",
      "iteration 50 ||                             Loss: 0.0886 ||                             10iter: 49.2204sec.\n",
      "iteration 60 ||                             Loss: 0.0729 ||                             10iter: 57.9064sec.\n",
      "iteration 70 ||                             Loss: 0.1165 ||                             10iter: 66.6232sec.\n",
      "iteration 80 ||                             Loss: 0.1351 ||                             10iter: 75.3311sec.\n",
      "iteration 90 ||                             Loss: 0.2174 ||                             10iter: 84.0095sec.\n",
      "iteration 100 ||                             Loss: 0.0904 ||                             10iter: 92.6768sec.\n",
      "iteration 110 ||                             Loss: 0.1408 ||                             10iter: 101.4430sec.\n",
      "iteration 120 ||                             Loss: 0.0670 ||                             10iter: 110.0915sec.\n",
      "iteration 130 ||                             Loss: 0.1251 ||                             10iter: 118.7936sec.\n",
      "iteration 140 ||                             Loss: 0.1471 ||                             10iter: 127.4854sec.\n",
      "iteration 150 ||                             Loss: 0.0795 ||                             10iter: 136.1365sec.\n",
      "iteration 160 ||                             Loss: 0.1436 ||                             10iter: 144.8661sec.\n",
      "iteration 170 ||                             Loss: 0.2100 ||                             10iter: 153.5674sec.\n",
      "iteration 180 ||                             Loss: 0.1434 ||                             10iter: 162.2456sec.\n",
      "------------\n",
      "epoch 1 || epoch_train_loss: 0.1771         || epoch_val_loss: 0.0000\n",
      "Epoch 2/30\n",
      " (train) \n",
      "iteration 190 ||                             Loss: 0.0996 ||                             10iter: 5.4535sec.\n",
      "iteration 200 ||                             Loss: 0.0867 ||                             10iter: 14.1791sec.\n",
      "iteration 210 ||                             Loss: 0.0706 ||                             10iter: 22.8660sec.\n",
      "iteration 220 ||                             Loss: 0.0455 ||                             10iter: 31.5134sec.\n",
      "iteration 230 ||                             Loss: 0.0542 ||                             10iter: 40.2177sec.\n",
      "iteration 240 ||                             Loss: 0.1100 ||                             10iter: 48.8950sec.\n",
      "iteration 250 ||                             Loss: 0.0663 ||                             10iter: 57.6263sec.\n",
      "iteration 260 ||                             Loss: 0.0587 ||                             10iter: 66.2972sec.\n",
      "iteration 270 ||                             Loss: 0.0649 ||                             10iter: 74.9410sec.\n",
      "iteration 280 ||                             Loss: 0.0505 ||                             10iter: 83.6392sec.\n",
      "iteration 290 ||                             Loss: 0.0584 ||                             10iter: 92.3394sec.\n",
      "iteration 300 ||                             Loss: 0.1098 ||                             10iter: 101.5078sec.\n",
      "iteration 310 ||                             Loss: 0.0826 ||                             10iter: 110.5787sec.\n",
      "iteration 320 ||                             Loss: 0.0724 ||                             10iter: 119.2591sec.\n",
      "iteration 330 ||                             Loss: 0.0507 ||                             10iter: 127.9844sec.\n",
      "iteration 340 ||                             Loss: 0.0828 ||                             10iter: 136.7084sec.\n",
      "iteration 350 ||                             Loss: 0.0687 ||                             10iter: 145.3968sec.\n",
      "iteration 360 ||                             Loss: 0.0571 ||                             10iter: 154.0412sec.\n",
      "------------\n",
      "epoch 2 || epoch_train_loss: 0.0919         || epoch_val_loss: 0.0000\n",
      "Epoch 3/30\n",
      " (train) \n",
      "iteration 370 ||                             Loss: 0.0671 ||                             10iter: 2.8881sec.\n",
      "iteration 380 ||                             Loss: 0.0642 ||                             10iter: 11.5571sec.\n",
      "iteration 390 ||                             Loss: 0.0568 ||                             10iter: 20.2708sec.\n",
      "iteration 400 ||                             Loss: 0.0435 ||                             10iter: 28.9193sec.\n",
      "iteration 410 ||                             Loss: 0.0571 ||                             10iter: 37.5897sec.\n",
      "iteration 420 ||                             Loss: 0.0940 ||                             10iter: 46.2574sec.\n",
      "iteration 430 ||                             Loss: 0.1126 ||                             10iter: 54.9819sec.\n",
      "iteration 440 ||                             Loss: 0.0753 ||                             10iter: 63.6913sec.\n",
      "iteration 450 ||                             Loss: 0.0613 ||                             10iter: 72.4043sec.\n",
      "iteration 460 ||                             Loss: 0.0666 ||                             10iter: 81.1516sec.\n",
      "iteration 470 ||                             Loss: 0.0577 ||                             10iter: 89.8446sec.\n",
      "iteration 480 ||                             Loss: 0.0470 ||                             10iter: 98.5184sec.\n",
      "iteration 490 ||                             Loss: 0.0734 ||                             10iter: 107.2522sec.\n",
      "iteration 500 ||                             Loss: 0.0639 ||                             10iter: 115.9398sec.\n",
      "iteration 510 ||                             Loss: 0.0772 ||                             10iter: 124.6651sec.\n",
      "iteration 520 ||                             Loss: 0.0846 ||                             10iter: 133.4688sec.\n",
      "iteration 530 ||                             Loss: 0.0612 ||                             10iter: 142.1584sec.\n",
      "iteration 540 ||                             Loss: 0.0438 ||                             10iter: 150.8306sec.\n",
      "------------\n",
      "epoch 3 || epoch_train_loss: 0.0784         || epoch_val_loss: 0.0000\n",
      "Epoch 4/30\n",
      " (train) \n",
      "iteration 550 ||                             Loss: 0.1615 ||                             10iter: 0.2423sec.\n",
      "iteration 560 ||                             Loss: 0.0374 ||                             10iter: 8.9420sec.\n",
      "iteration 570 ||                             Loss: 0.0570 ||                             10iter: 17.5955sec.\n",
      "iteration 580 ||                             Loss: 0.0982 ||                             10iter: 26.2631sec.\n",
      "iteration 590 ||                             Loss: 0.0680 ||                             10iter: 34.9539sec.\n",
      "iteration 600 ||                             Loss: 0.0669 ||                             10iter: 43.6737sec.\n",
      "iteration 610 ||                             Loss: 0.0639 ||                             10iter: 52.3611sec.\n",
      "iteration 620 ||                             Loss: 0.0277 ||                             10iter: 61.0381sec.\n",
      "iteration 630 ||                             Loss: 0.1326 ||                             10iter: 69.7490sec.\n",
      "iteration 640 ||                             Loss: 0.0860 ||                             10iter: 78.4390sec.\n",
      "iteration 650 ||                             Loss: 0.0637 ||                             10iter: 87.1188sec.\n",
      "iteration 660 ||                             Loss: 0.0748 ||                             10iter: 95.8460sec.\n",
      "iteration 670 ||                             Loss: 0.0435 ||                             10iter: 104.9458sec.\n",
      "iteration 680 ||                             Loss: 0.1265 ||                             10iter: 113.6585sec.\n",
      "iteration 690 ||                             Loss: 0.0849 ||                             10iter: 122.3345sec.\n",
      "iteration 700 ||                             Loss: 0.0771 ||                             10iter: 131.6540sec.\n",
      "iteration 710 ||                             Loss: 0.0654 ||                             10iter: 140.3935sec.\n",
      "iteration 720 ||                             Loss: 0.0438 ||                             10iter: 149.0481sec.\n",
      "iteration 730 ||                             Loss: 0.0662 ||                             10iter: 157.7271sec.\n",
      "------------\n",
      "epoch 4 || epoch_train_loss: 0.0715         || epoch_val_loss: 0.0000\n",
      "Epoch 5/30\n",
      " (train) \n",
      "iteration 740 ||                             Loss: 0.0746 ||                             10iter: 6.3165sec.\n",
      "iteration 750 ||                             Loss: 0.0988 ||                             10iter: 15.3073sec.\n",
      "iteration 760 ||                             Loss: 0.1140 ||                             10iter: 24.0136sec.\n",
      "iteration 770 ||                             Loss: 0.0327 ||                             10iter: 32.7570sec.\n",
      "iteration 780 ||                             Loss: 0.0714 ||                             10iter: 41.4614sec.\n",
      "iteration 790 ||                             Loss: 0.0829 ||                             10iter: 50.1556sec.\n",
      "iteration 800 ||                             Loss: 0.0667 ||                             10iter: 58.8730sec.\n",
      "iteration 810 ||                             Loss: 0.0518 ||                             10iter: 67.5371sec.\n",
      "iteration 820 ||                             Loss: 0.0584 ||                             10iter: 76.1997sec.\n",
      "iteration 830 ||                             Loss: 0.0917 ||                             10iter: 84.8954sec.\n",
      "iteration 840 ||                             Loss: 0.0988 ||                             10iter: 93.6452sec.\n",
      "iteration 850 ||                             Loss: 0.1132 ||                             10iter: 102.3254sec.\n",
      "iteration 860 ||                             Loss: 0.0351 ||                             10iter: 110.9980sec.\n",
      "iteration 870 ||                             Loss: 0.0751 ||                             10iter: 119.6676sec.\n",
      "iteration 880 ||                             Loss: 0.0333 ||                             10iter: 128.3462sec.\n",
      "iteration 890 ||                             Loss: 0.0354 ||                             10iter: 137.0476sec.\n",
      "iteration 900 ||                             Loss: 0.0354 ||                             10iter: 145.6937sec.\n",
      "iteration 910 ||                             Loss: 0.0548 ||                             10iter: 154.3685sec.\n",
      "---------\n",
      " (val) \n",
      "------------\n",
      "epoch 5 || epoch_train_loss: 0.0667         || epoch_val_loss: 0.0802\n",
      "Epoch 6/30\n",
      " (train) \n",
      "iteration 920 ||                             Loss: 0.0560 ||                             10iter: 3.8361sec.\n",
      "iteration 930 ||                             Loss: 0.0487 ||                             10iter: 12.4747sec.\n",
      "iteration 940 ||                             Loss: 0.0908 ||                             10iter: 21.1948sec.\n",
      "iteration 950 ||                             Loss: 0.0613 ||                             10iter: 29.9237sec.\n",
      "iteration 960 ||                             Loss: 0.0578 ||                             10iter: 38.6005sec.\n",
      "iteration 970 ||                             Loss: 0.0488 ||                             10iter: 47.2812sec.\n",
      "iteration 980 ||                             Loss: 0.0814 ||                             10iter: 56.0065sec.\n",
      "iteration 990 ||                             Loss: 0.0917 ||                             10iter: 64.7185sec.\n",
      "iteration 1000 ||                             Loss: 0.1494 ||                             10iter: 73.4059sec.\n",
      "iteration 1010 ||                             Loss: 0.0452 ||                             10iter: 82.0855sec.\n",
      "iteration 1020 ||                             Loss: 0.0670 ||                             10iter: 90.7621sec.\n",
      "iteration 1030 ||                             Loss: 0.0590 ||                             10iter: 99.4817sec.\n",
      "iteration 1040 ||                             Loss: 0.0650 ||                             10iter: 108.1540sec.\n",
      "iteration 1050 ||                             Loss: 0.0576 ||                             10iter: 116.9007sec.\n",
      "iteration 1060 ||                             Loss: 0.0706 ||                             10iter: 125.6314sec.\n",
      "iteration 1070 ||                             Loss: 0.0383 ||                             10iter: 134.3009sec.\n",
      "iteration 1080 ||                             Loss: 0.0497 ||                             10iter: 143.0464sec.\n",
      "iteration 1090 ||                             Loss: 0.0606 ||                             10iter: 151.7706sec.\n",
      "------------\n",
      "epoch 6 || epoch_train_loss: 0.0612         || epoch_val_loss: 0.0000\n",
      "Epoch 7/30\n",
      " (train) \n",
      "iteration 1100 ||                             Loss: 0.0618 ||                             10iter: 1.1086sec.\n",
      "iteration 1110 ||                             Loss: 0.1202 ||                             10iter: 9.8260sec.\n",
      "iteration 1120 ||                             Loss: 0.0708 ||                             10iter: 18.5488sec.\n",
      "iteration 1130 ||                             Loss: 0.0753 ||                             10iter: 27.2680sec.\n",
      "iteration 1140 ||                             Loss: 0.0829 ||                             10iter: 35.9535sec.\n",
      "iteration 1150 ||                             Loss: 0.0363 ||                             10iter: 44.6805sec.\n",
      "iteration 1160 ||                             Loss: 0.0609 ||                             10iter: 53.3758sec.\n",
      "iteration 1170 ||                             Loss: 0.0541 ||                             10iter: 62.1318sec.\n",
      "iteration 1180 ||                             Loss: 0.0515 ||                             10iter: 70.8605sec.\n",
      "iteration 1190 ||                             Loss: 0.0933 ||                             10iter: 79.5737sec.\n",
      "iteration 1200 ||                             Loss: 0.0620 ||                             10iter: 88.3102sec.\n",
      "iteration 1210 ||                             Loss: 0.0687 ||                             10iter: 97.0156sec.\n",
      "iteration 1220 ||                             Loss: 0.0632 ||                             10iter: 105.6980sec.\n",
      "iteration 1230 ||                             Loss: 0.0509 ||                             10iter: 114.3887sec.\n",
      "iteration 1240 ||                             Loss: 0.0583 ||                             10iter: 123.0727sec.\n",
      "iteration 1250 ||                             Loss: 0.0475 ||                             10iter: 131.7419sec.\n",
      "iteration 1260 ||                             Loss: 0.0974 ||                             10iter: 140.4437sec.\n",
      "iteration 1270 ||                             Loss: 0.0375 ||                             10iter: 149.1260sec.\n",
      "iteration 1280 ||                             Loss: 0.0695 ||                             10iter: 157.8345sec.\n",
      "------------\n",
      "epoch 7 || epoch_train_loss: 0.0602         || epoch_val_loss: 0.0000\n",
      "Epoch 8/30\n",
      " (train) \n",
      "iteration 1290 ||                             Loss: 0.0407 ||                             10iter: 7.2066sec.\n",
      "iteration 1300 ||                             Loss: 0.0530 ||                             10iter: 15.8964sec.\n",
      "iteration 1310 ||                             Loss: 0.0378 ||                             10iter: 24.5559sec.\n",
      "iteration 1320 ||                             Loss: 0.0909 ||                             10iter: 33.2439sec.\n",
      "iteration 1330 ||                             Loss: 0.0578 ||                             10iter: 41.9539sec.\n",
      "iteration 1340 ||                             Loss: 0.0601 ||                             10iter: 50.7234sec.\n",
      "iteration 1350 ||                             Loss: 0.0648 ||                             10iter: 59.4469sec.\n",
      "iteration 1360 ||                             Loss: 0.0702 ||                             10iter: 68.1460sec.\n",
      "iteration 1370 ||                             Loss: 0.0792 ||                             10iter: 76.8629sec.\n",
      "iteration 1380 ||                             Loss: 0.0717 ||                             10iter: 85.5453sec.\n",
      "iteration 1390 ||                             Loss: 0.0788 ||                             10iter: 94.2110sec.\n",
      "iteration 1400 ||                             Loss: 0.0593 ||                             10iter: 102.9100sec.\n",
      "iteration 1410 ||                             Loss: 0.0686 ||                             10iter: 111.6150sec.\n",
      "iteration 1420 ||                             Loss: 0.0747 ||                             10iter: 120.3483sec.\n",
      "iteration 1430 ||                             Loss: 0.1078 ||                             10iter: 129.0898sec.\n",
      "iteration 1440 ||                             Loss: 0.0647 ||                             10iter: 137.8426sec.\n",
      "iteration 1450 ||                             Loss: 0.0558 ||                             10iter: 146.5749sec.\n",
      "iteration 1460 ||                             Loss: 0.0373 ||                             10iter: 155.2790sec.\n",
      "------------\n",
      "epoch 8 || epoch_train_loss: 0.0574         || epoch_val_loss: 0.0000\n",
      "Epoch 9/30\n",
      " (train) \n",
      "iteration 1470 ||                             Loss: 0.0470 ||                             10iter: 4.6129sec.\n",
      "iteration 1480 ||                             Loss: 0.0208 ||                             10iter: 13.3032sec.\n",
      "iteration 1490 ||                             Loss: 0.0394 ||                             10iter: 21.9765sec.\n",
      "iteration 1500 ||                             Loss: 0.0465 ||                             10iter: 30.6644sec.\n",
      "iteration 1510 ||                             Loss: 0.0371 ||                             10iter: 39.3944sec.\n",
      "iteration 1520 ||                             Loss: 0.0362 ||                             10iter: 48.0709sec.\n",
      "iteration 1530 ||                             Loss: 0.1007 ||                             10iter: 56.7617sec.\n",
      "iteration 1540 ||                             Loss: 0.0511 ||                             10iter: 65.4403sec.\n",
      "iteration 1550 ||                             Loss: 0.0860 ||                             10iter: 74.1556sec.\n",
      "iteration 1560 ||                             Loss: 0.0861 ||                             10iter: 82.9210sec.\n",
      "iteration 1570 ||                             Loss: 0.0449 ||                             10iter: 91.6171sec.\n",
      "iteration 1580 ||                             Loss: 0.0600 ||                             10iter: 100.3977sec.\n",
      "iteration 1590 ||                             Loss: 0.0566 ||                             10iter: 109.1011sec.\n",
      "iteration 1600 ||                             Loss: 0.0466 ||                             10iter: 117.8277sec.\n",
      "iteration 1610 ||                             Loss: 0.0457 ||                             10iter: 126.5444sec.\n",
      "iteration 1620 ||                             Loss: 0.0619 ||                             10iter: 135.2566sec.\n",
      "iteration 1630 ||                             Loss: 0.0550 ||                             10iter: 143.9683sec.\n",
      "iteration 1640 ||                             Loss: 0.0414 ||                             10iter: 152.6430sec.\n",
      "------------\n",
      "epoch 9 || epoch_train_loss: 0.0545         || epoch_val_loss: 0.0000\n",
      "Epoch 10/30\n",
      " (train) \n",
      "iteration 1650 ||                             Loss: 0.0490 ||                             10iter: 2.1196sec.\n",
      "iteration 1660 ||                             Loss: 0.0452 ||                             10iter: 10.9272sec.\n",
      "iteration 1670 ||                             Loss: 0.0631 ||                             10iter: 19.5705sec.\n",
      "iteration 1680 ||                             Loss: 0.0411 ||                             10iter: 28.3122sec.\n",
      "iteration 1690 ||                             Loss: 0.0477 ||                             10iter: 37.0375sec.\n",
      "iteration 1700 ||                             Loss: 0.0685 ||                             10iter: 45.7395sec.\n",
      "iteration 1710 ||                             Loss: 0.0396 ||                             10iter: 54.4440sec.\n",
      "iteration 1720 ||                             Loss: 0.0526 ||                             10iter: 63.0984sec.\n",
      "iteration 1730 ||                             Loss: 0.0468 ||                             10iter: 71.7797sec.\n",
      "iteration 1740 ||                             Loss: 0.0421 ||                             10iter: 80.4529sec.\n",
      "iteration 1750 ||                             Loss: 0.0640 ||                             10iter: 89.1138sec.\n",
      "iteration 1760 ||                             Loss: 0.0586 ||                             10iter: 97.8131sec.\n",
      "iteration 1770 ||                             Loss: 0.0694 ||                             10iter: 106.4876sec.\n",
      "iteration 1780 ||                             Loss: 0.0540 ||                             10iter: 115.1874sec.\n",
      "iteration 1790 ||                             Loss: 0.0377 ||                             10iter: 124.0642sec.\n",
      "iteration 1800 ||                             Loss: 0.0408 ||                             10iter: 132.8085sec.\n",
      "iteration 1810 ||                             Loss: 0.0582 ||                             10iter: 141.5201sec.\n",
      "iteration 1820 ||                             Loss: 0.0626 ||                             10iter: 150.1949sec.\n",
      "iteration 1830 ||                             Loss: 0.0794 ||                             10iter: 158.8478sec.\n",
      "---------\n",
      " (val) \n",
      "------------\n",
      "epoch 10 || epoch_train_loss: 0.0505         || epoch_val_loss: 0.0759\n",
      "Epoch 11/30\n",
      " (train) \n",
      "iteration 1840 ||                             Loss: 0.0756 ||                             10iter: 8.0078sec.\n",
      "iteration 1850 ||                             Loss: 0.0558 ||                             10iter: 16.7343sec.\n",
      "iteration 1860 ||                             Loss: 0.0391 ||                             10iter: 25.4035sec.\n",
      "iteration 1870 ||                             Loss: 0.0485 ||                             10iter: 34.1010sec.\n",
      "iteration 1880 ||                             Loss: 0.0455 ||                             10iter: 42.7899sec.\n",
      "iteration 1890 ||                             Loss: 0.0524 ||                             10iter: 51.4663sec.\n",
      "iteration 1900 ||                             Loss: 0.0644 ||                             10iter: 60.1550sec.\n",
      "iteration 1910 ||                             Loss: 0.0500 ||                             10iter: 68.8092sec.\n",
      "iteration 1920 ||                             Loss: 0.0512 ||                             10iter: 77.4852sec.\n",
      "iteration 1930 ||                             Loss: 0.0395 ||                             10iter: 86.1686sec.\n",
      "iteration 1940 ||                             Loss: 0.0406 ||                             10iter: 94.9063sec.\n",
      "iteration 1950 ||                             Loss: 0.0350 ||                             10iter: 103.6235sec.\n",
      "iteration 1960 ||                             Loss: 0.0714 ||                             10iter: 112.3184sec.\n",
      "iteration 1970 ||                             Loss: 0.0374 ||                             10iter: 121.0377sec.\n",
      "iteration 1980 ||                             Loss: 0.0327 ||                             10iter: 129.7127sec.\n",
      "iteration 1990 ||                             Loss: 0.0377 ||                             10iter: 138.3960sec.\n",
      "iteration 2000 ||                             Loss: 0.0606 ||                             10iter: 147.0883sec.\n",
      "iteration 2010 ||                             Loss: 0.0384 ||                             10iter: 155.8148sec.\n",
      "------------\n",
      "epoch 11 || epoch_train_loss: 0.0496         || epoch_val_loss: 0.0000\n",
      "Epoch 12/30\n",
      " (train) \n",
      "iteration 2020 ||                             Loss: 0.0458 ||                             10iter: 5.4458sec.\n",
      "iteration 2030 ||                             Loss: 0.0392 ||                             10iter: 14.1235sec.\n",
      "iteration 2040 ||                             Loss: 0.0485 ||                             10iter: 22.7428sec.\n",
      "iteration 2050 ||                             Loss: 0.0554 ||                             10iter: 31.4319sec.\n",
      "iteration 2060 ||                             Loss: 0.0343 ||                             10iter: 40.1003sec.\n",
      "iteration 2070 ||                             Loss: 0.0444 ||                             10iter: 48.7778sec.\n",
      "iteration 2080 ||                             Loss: 0.0355 ||                             10iter: 58.0772sec.\n",
      "iteration 2090 ||                             Loss: 0.0642 ||                             10iter: 66.7320sec.\n",
      "iteration 2100 ||                             Loss: 0.0589 ||                             10iter: 75.4773sec.\n",
      "iteration 2110 ||                             Loss: 0.0692 ||                             10iter: 84.2498sec.\n",
      "iteration 2120 ||                             Loss: 0.0560 ||                             10iter: 93.0445sec.\n",
      "iteration 2130 ||                             Loss: 0.0419 ||                             10iter: 101.7726sec.\n",
      "iteration 2140 ||                             Loss: 0.0331 ||                             10iter: 110.4958sec.\n",
      "iteration 2150 ||                             Loss: 0.0253 ||                             10iter: 119.3168sec.\n",
      "iteration 2160 ||                             Loss: 0.0390 ||                             10iter: 128.4597sec.\n",
      "iteration 2170 ||                             Loss: 0.1635 ||                             10iter: 137.1817sec.\n",
      "iteration 2180 ||                             Loss: 0.0539 ||                             10iter: 145.8409sec.\n",
      "iteration 2190 ||                             Loss: 0.0371 ||                             10iter: 154.4931sec.\n",
      "------------\n",
      "epoch 12 || epoch_train_loss: 0.0484         || epoch_val_loss: 0.0000\n",
      "Epoch 13/30\n",
      " (train) \n",
      "iteration 2200 ||                             Loss: 0.0593 ||                             10iter: 2.8250sec.\n",
      "iteration 2210 ||                             Loss: 0.0508 ||                             10iter: 11.4730sec.\n",
      "iteration 2220 ||                             Loss: 0.0592 ||                             10iter: 20.1468sec.\n",
      "iteration 2230 ||                             Loss: 0.0516 ||                             10iter: 28.8852sec.\n",
      "iteration 2240 ||                             Loss: 0.0385 ||                             10iter: 37.5818sec.\n",
      "iteration 2250 ||                             Loss: 0.0420 ||                             10iter: 46.2536sec.\n",
      "iteration 2260 ||                             Loss: 0.0401 ||                             10iter: 54.9649sec.\n",
      "iteration 2270 ||                             Loss: 0.0354 ||                             10iter: 63.6847sec.\n",
      "iteration 2280 ||                             Loss: 0.0460 ||                             10iter: 72.3272sec.\n",
      "iteration 2290 ||                             Loss: 0.0320 ||                             10iter: 81.0134sec.\n",
      "iteration 2300 ||                             Loss: 0.0438 ||                             10iter: 89.7253sec.\n",
      "iteration 2310 ||                             Loss: 0.0342 ||                             10iter: 98.3703sec.\n",
      "iteration 2320 ||                             Loss: 0.0472 ||                             10iter: 107.0858sec.\n",
      "iteration 2330 ||                             Loss: 0.0268 ||                             10iter: 115.7540sec.\n",
      "iteration 2340 ||                             Loss: 0.0516 ||                             10iter: 124.4168sec.\n",
      "iteration 2350 ||                             Loss: 0.0263 ||                             10iter: 133.0938sec.\n",
      "iteration 2360 ||                             Loss: 0.0443 ||                             10iter: 141.7803sec.\n",
      "iteration 2370 ||                             Loss: 0.0937 ||                             10iter: 150.4818sec.\n",
      "------------\n",
      "epoch 13 || epoch_train_loss: 0.0467         || epoch_val_loss: 0.0000\n",
      "Epoch 14/30\n",
      " (train) \n",
      "iteration 2380 ||                             Loss: 0.0217 ||                             10iter: 0.2438sec.\n",
      "iteration 2390 ||                             Loss: 0.0371 ||                             10iter: 8.9037sec.\n",
      "iteration 2400 ||                             Loss: 0.0380 ||                             10iter: 17.6007sec.\n",
      "iteration 2410 ||                             Loss: 0.0335 ||                             10iter: 26.2679sec.\n",
      "iteration 2420 ||                             Loss: 0.0488 ||                             10iter: 34.9587sec.\n",
      "iteration 2430 ||                             Loss: 0.0558 ||                             10iter: 43.6657sec.\n",
      "iteration 2440 ||                             Loss: 0.0583 ||                             10iter: 52.3944sec.\n",
      "iteration 2450 ||                             Loss: 0.0609 ||                             10iter: 61.0889sec.\n",
      "iteration 2460 ||                             Loss: 0.0418 ||                             10iter: 69.8213sec.\n",
      "iteration 2470 ||                             Loss: 0.0424 ||                             10iter: 78.4905sec.\n",
      "iteration 2480 ||                             Loss: 0.0307 ||                             10iter: 87.1764sec.\n",
      "iteration 2490 ||                             Loss: 0.0465 ||                             10iter: 95.8159sec.\n",
      "iteration 2500 ||                             Loss: 0.0547 ||                             10iter: 104.5160sec.\n",
      "iteration 2510 ||                             Loss: 0.0433 ||                             10iter: 113.1675sec.\n",
      "iteration 2520 ||                             Loss: 0.0439 ||                             10iter: 121.8489sec.\n",
      "iteration 2530 ||                             Loss: 0.0764 ||                             10iter: 130.5595sec.\n",
      "iteration 2540 ||                             Loss: 0.0458 ||                             10iter: 139.2301sec.\n",
      "iteration 2550 ||                             Loss: 0.0497 ||                             10iter: 147.9166sec.\n",
      "iteration 2560 ||                             Loss: 0.0285 ||                             10iter: 156.6326sec.\n",
      "------------\n",
      "epoch 14 || epoch_train_loss: 0.0463         || epoch_val_loss: 0.0000\n",
      "Epoch 15/30\n",
      " (train) \n",
      "iteration 2570 ||                             Loss: 0.0356 ||                             10iter: 6.3166sec.\n",
      "iteration 2580 ||                             Loss: 0.0422 ||                             10iter: 15.0138sec.\n",
      "iteration 2590 ||                             Loss: 0.0312 ||                             10iter: 23.6422sec.\n",
      "iteration 2600 ||                             Loss: 0.0341 ||                             10iter: 32.3485sec.\n",
      "iteration 2610 ||                             Loss: 0.0455 ||                             10iter: 41.0249sec.\n",
      "iteration 2620 ||                             Loss: 0.0265 ||                             10iter: 49.7064sec.\n",
      "iteration 2630 ||                             Loss: 0.0341 ||                             10iter: 58.3512sec.\n",
      "iteration 2640 ||                             Loss: 0.0613 ||                             10iter: 67.0606sec.\n",
      "iteration 2650 ||                             Loss: 0.0496 ||                             10iter: 75.7504sec.\n",
      "iteration 2660 ||                             Loss: 0.0522 ||                             10iter: 84.4800sec.\n",
      "iteration 2670 ||                             Loss: 0.0372 ||                             10iter: 93.1401sec.\n",
      "iteration 2680 ||                             Loss: 0.0397 ||                             10iter: 101.8331sec.\n",
      "iteration 2690 ||                             Loss: 0.0268 ||                             10iter: 110.5943sec.\n",
      "iteration 2700 ||                             Loss: 0.0380 ||                             10iter: 119.3161sec.\n",
      "iteration 2710 ||                             Loss: 0.0571 ||                             10iter: 128.0475sec.\n",
      "iteration 2720 ||                             Loss: 0.0337 ||                             10iter: 136.7465sec.\n",
      "iteration 2730 ||                             Loss: 0.0480 ||                             10iter: 145.4223sec.\n",
      "iteration 2740 ||                             Loss: 0.0270 ||                             10iter: 154.1556sec.\n",
      "---------\n",
      " (val) \n",
      "------------\n",
      "epoch 15 || epoch_train_loss: 0.0459         || epoch_val_loss: 0.0714\n",
      "Epoch 16/30\n",
      " (train) \n",
      "iteration 2750 ||                             Loss: 0.0727 ||                             10iter: 3.7320sec.\n",
      "iteration 2760 ||                             Loss: 0.0295 ||                             10iter: 12.4356sec.\n",
      "iteration 2770 ||                             Loss: 0.0294 ||                             10iter: 21.2099sec.\n",
      "iteration 2780 ||                             Loss: 0.0383 ||                             10iter: 29.9120sec.\n",
      "iteration 2790 ||                             Loss: 0.0383 ||                             10iter: 38.6088sec.\n",
      "iteration 2800 ||                             Loss: 0.0466 ||                             10iter: 47.2727sec.\n",
      "iteration 2810 ||                             Loss: 0.0319 ||                             10iter: 55.9875sec.\n",
      "iteration 2820 ||                             Loss: 0.0308 ||                             10iter: 64.6667sec.\n",
      "iteration 2830 ||                             Loss: 0.0481 ||                             10iter: 73.3525sec.\n",
      "iteration 2840 ||                             Loss: 0.0259 ||                             10iter: 81.9980sec.\n",
      "iteration 2850 ||                             Loss: 0.0436 ||                             10iter: 90.6556sec.\n",
      "iteration 2860 ||                             Loss: 0.0508 ||                             10iter: 99.4984sec.\n",
      "iteration 2870 ||                             Loss: 0.0292 ||                             10iter: 108.2119sec.\n",
      "iteration 2880 ||                             Loss: 0.0292 ||                             10iter: 116.8587sec.\n",
      "iteration 2890 ||                             Loss: 0.0362 ||                             10iter: 125.5578sec.\n",
      "iteration 2900 ||                             Loss: 0.0359 ||                             10iter: 134.2686sec.\n",
      "iteration 2910 ||                             Loss: 0.0402 ||                             10iter: 142.9568sec.\n",
      "iteration 2920 ||                             Loss: 0.0461 ||                             10iter: 151.6124sec.\n",
      "------------\n",
      "epoch 16 || epoch_train_loss: 0.0429         || epoch_val_loss: 0.0000\n",
      "Epoch 17/30\n",
      " (train) \n",
      "iteration 2930 ||                             Loss: 0.0508 ||                             10iter: 1.1129sec.\n",
      "iteration 2940 ||                             Loss: 0.0508 ||                             10iter: 9.7624sec.\n",
      "iteration 2950 ||                             Loss: 0.0486 ||                             10iter: 18.4964sec.\n",
      "iteration 2960 ||                             Loss: 0.0533 ||                             10iter: 27.1858sec.\n",
      "iteration 2970 ||                             Loss: 0.0416 ||                             10iter: 35.8375sec.\n",
      "iteration 2980 ||                             Loss: 0.0523 ||                             10iter: 44.6718sec.\n",
      "iteration 2990 ||                             Loss: 0.0521 ||                             10iter: 53.4206sec.\n",
      "iteration 3000 ||                             Loss: 0.0402 ||                             10iter: 62.1444sec.\n",
      "iteration 3010 ||                             Loss: 0.0366 ||                             10iter: 70.8521sec.\n",
      "iteration 3020 ||                             Loss: 0.0262 ||                             10iter: 79.5580sec.\n",
      "iteration 3030 ||                             Loss: 0.0471 ||                             10iter: 88.2226sec.\n",
      "iteration 3040 ||                             Loss: 0.0384 ||                             10iter: 96.9039sec.\n",
      "iteration 3050 ||                             Loss: 0.0590 ||                             10iter: 105.6057sec.\n",
      "iteration 3060 ||                             Loss: 0.0270 ||                             10iter: 114.2743sec.\n",
      "iteration 3070 ||                             Loss: 0.0282 ||                             10iter: 123.0053sec.\n",
      "iteration 3080 ||                             Loss: 0.0697 ||                             10iter: 131.9015sec.\n",
      "iteration 3090 ||                             Loss: 0.0361 ||                             10iter: 140.5793sec.\n",
      "iteration 3100 ||                             Loss: 0.0488 ||                             10iter: 149.2636sec.\n",
      "iteration 3110 ||                             Loss: 0.0508 ||                             10iter: 158.0127sec.\n",
      "------------\n",
      "epoch 17 || epoch_train_loss: 0.0449         || epoch_val_loss: 0.0000\n",
      "Epoch 18/30\n",
      " (train) \n",
      "iteration 3120 ||                             Loss: 0.0292 ||                             10iter: 7.1905sec.\n",
      "iteration 3130 ||                             Loss: 0.0469 ||                             10iter: 15.9185sec.\n",
      "iteration 3140 ||                             Loss: 0.0338 ||                             10iter: 24.6385sec.\n",
      "iteration 3150 ||                             Loss: 0.0506 ||                             10iter: 33.3357sec.\n",
      "iteration 3160 ||                             Loss: 0.0281 ||                             10iter: 42.0333sec.\n",
      "iteration 3170 ||                             Loss: 0.0269 ||                             10iter: 50.7047sec.\n",
      "iteration 3180 ||                             Loss: 0.0320 ||                             10iter: 59.3253sec.\n",
      "iteration 3190 ||                             Loss: 0.0420 ||                             10iter: 67.9788sec.\n",
      "iteration 3200 ||                             Loss: 0.0972 ||                             10iter: 76.6605sec.\n",
      "iteration 3210 ||                             Loss: 0.0425 ||                             10iter: 85.4018sec.\n",
      "iteration 3220 ||                             Loss: 0.0345 ||                             10iter: 94.0637sec.\n",
      "iteration 3230 ||                             Loss: 0.0318 ||                             10iter: 102.7270sec.\n",
      "iteration 3240 ||                             Loss: 0.0424 ||                             10iter: 111.3949sec.\n",
      "iteration 3250 ||                             Loss: 0.0499 ||                             10iter: 120.0914sec.\n",
      "iteration 3260 ||                             Loss: 0.0426 ||                             10iter: 128.7607sec.\n",
      "iteration 3270 ||                             Loss: 0.0514 ||                             10iter: 137.4455sec.\n",
      "iteration 3280 ||                             Loss: 0.0328 ||                             10iter: 146.1528sec.\n",
      "iteration 3290 ||                             Loss: 0.0363 ||                             10iter: 154.8268sec.\n",
      "------------\n",
      "epoch 18 || epoch_train_loss: 0.0437         || epoch_val_loss: 0.0000\n",
      "Epoch 19/30\n",
      " (train) \n",
      "iteration 3300 ||                             Loss: 0.0450 ||                             10iter: 4.6085sec.\n",
      "iteration 3310 ||                             Loss: 0.0860 ||                             10iter: 13.3046sec.\n",
      "iteration 3320 ||                             Loss: 0.0480 ||                             10iter: 21.9709sec.\n",
      "iteration 3330 ||                             Loss: 0.0296 ||                             10iter: 30.6408sec.\n",
      "iteration 3340 ||                             Loss: 0.0648 ||                             10iter: 39.3883sec.\n",
      "iteration 3350 ||                             Loss: 0.0440 ||                             10iter: 48.0528sec.\n",
      "iteration 3360 ||                             Loss: 0.0393 ||                             10iter: 56.7390sec.\n",
      "iteration 3370 ||                             Loss: 0.0300 ||                             10iter: 65.4428sec.\n",
      "iteration 3380 ||                             Loss: 0.0335 ||                             10iter: 74.1452sec.\n",
      "iteration 3390 ||                             Loss: 0.0306 ||                             10iter: 82.8084sec.\n",
      "iteration 3400 ||                             Loss: 0.0367 ||                             10iter: 91.5146sec.\n",
      "iteration 3410 ||                             Loss: 0.0366 ||                             10iter: 100.2194sec.\n",
      "iteration 3420 ||                             Loss: 0.0485 ||                             10iter: 108.8841sec.\n",
      "iteration 3430 ||                             Loss: 0.0385 ||                             10iter: 117.5551sec.\n",
      "iteration 3440 ||                             Loss: 0.0502 ||                             10iter: 126.2409sec.\n",
      "iteration 3450 ||                             Loss: 0.0317 ||                             10iter: 134.9603sec.\n",
      "iteration 3460 ||                             Loss: 0.0296 ||                             10iter: 143.6349sec.\n",
      "iteration 3470 ||                             Loss: 0.0399 ||                             10iter: 152.3533sec.\n",
      "------------\n",
      "epoch 19 || epoch_train_loss: 0.0435         || epoch_val_loss: 0.0000\n",
      "Epoch 20/30\n",
      " (train) \n",
      "iteration 3480 ||                             Loss: 0.0246 ||                             10iter: 1.9917sec.\n",
      "iteration 3490 ||                             Loss: 0.0380 ||                             10iter: 10.5994sec.\n",
      "iteration 3500 ||                             Loss: 0.0592 ||                             10iter: 19.2834sec.\n",
      "iteration 3510 ||                             Loss: 0.0312 ||                             10iter: 27.9583sec.\n",
      "iteration 3520 ||                             Loss: 0.0202 ||                             10iter: 36.6773sec.\n",
      "iteration 3530 ||                             Loss: 0.0424 ||                             10iter: 45.3527sec.\n",
      "iteration 3540 ||                             Loss: 0.0383 ||                             10iter: 54.0797sec.\n",
      "iteration 3550 ||                             Loss: 0.0472 ||                             10iter: 62.8296sec.\n",
      "iteration 3560 ||                             Loss: 0.0688 ||                             10iter: 71.5082sec.\n",
      "iteration 3570 ||                             Loss: 0.0342 ||                             10iter: 80.1918sec.\n",
      "iteration 3580 ||                             Loss: 0.0343 ||                             10iter: 88.8931sec.\n",
      "iteration 3590 ||                             Loss: 0.0409 ||                             10iter: 97.5429sec.\n",
      "iteration 3600 ||                             Loss: 0.0802 ||                             10iter: 106.2571sec.\n",
      "iteration 3610 ||                             Loss: 0.0521 ||                             10iter: 114.9786sec.\n",
      "iteration 3620 ||                             Loss: 0.0520 ||                             10iter: 123.6713sec.\n",
      "iteration 3630 ||                             Loss: 0.0221 ||                             10iter: 132.3010sec.\n",
      "iteration 3640 ||                             Loss: 0.0224 ||                             10iter: 141.0306sec.\n",
      "iteration 3650 ||                             Loss: 0.0512 ||                             10iter: 149.6789sec.\n",
      "iteration 3660 ||                             Loss: 0.0398 ||                             10iter: 158.3164sec.\n",
      "---------\n",
      " (val) \n",
      "------------\n",
      "epoch 20 || epoch_train_loss: 0.0433         || epoch_val_loss: 0.0714\n",
      "Epoch 21/30\n",
      " (train) \n",
      "iteration 3670 ||                             Loss: 0.0729 ||                             10iter: 8.0779sec.\n",
      "iteration 3680 ||                             Loss: 0.0626 ||                             10iter: 16.7904sec.\n",
      "iteration 3690 ||                             Loss: 0.0331 ||                             10iter: 25.5061sec.\n",
      "iteration 3700 ||                             Loss: 0.0233 ||                             10iter: 34.1783sec.\n",
      "iteration 3710 ||                             Loss: 0.0395 ||                             10iter: 42.8715sec.\n",
      "iteration 3720 ||                             Loss: 0.0430 ||                             10iter: 51.5442sec.\n",
      "iteration 3730 ||                             Loss: 0.0496 ||                             10iter: 60.2136sec.\n",
      "iteration 3740 ||                             Loss: 0.0337 ||                             10iter: 68.9073sec.\n",
      "iteration 3750 ||                             Loss: 0.0486 ||                             10iter: 77.5928sec.\n",
      "iteration 3760 ||                             Loss: 0.0252 ||                             10iter: 86.2329sec.\n",
      "iteration 3770 ||                             Loss: 0.0524 ||                             10iter: 94.8785sec.\n",
      "iteration 3780 ||                             Loss: 0.0667 ||                             10iter: 103.6033sec.\n",
      "iteration 3790 ||                             Loss: 0.0347 ||                             10iter: 112.2766sec.\n",
      "iteration 3800 ||                             Loss: 0.0535 ||                             10iter: 120.9575sec.\n",
      "iteration 3810 ||                             Loss: 0.0384 ||                             10iter: 129.6635sec.\n",
      "iteration 3820 ||                             Loss: 0.0487 ||                             10iter: 138.3646sec.\n",
      "iteration 3830 ||                             Loss: 0.0416 ||                             10iter: 147.0600sec.\n",
      "iteration 3840 ||                             Loss: 0.0225 ||                             10iter: 155.7994sec.\n",
      "------------\n",
      "epoch 21 || epoch_train_loss: 0.0422         || epoch_val_loss: 0.0000\n",
      "Epoch 22/30\n",
      " (train) \n",
      "iteration 3850 ||                             Loss: 0.0242 ||                             10iter: 5.4391sec.\n",
      "iteration 3860 ||                             Loss: 0.0608 ||                             10iter: 14.1163sec.\n",
      "iteration 3870 ||                             Loss: 0.0239 ||                             10iter: 22.8333sec.\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 30\n",
    "train_model(net, dataloaders_dict, criterion, scheduler, optimizer, num_epochs=num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:new]",
   "language": "python",
   "name": "conda-env-new-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
